{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b840bc4-7ae9-43be-a749-3a82825b5c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import dataloader,dataset\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision.utils import save_image\n",
    "from torch.distributions.normal import Normal\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.transforms import ToTensor \n",
    "from torchsummary import summary\n",
    "\n",
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.io import imread, imshow\n",
    "\n",
    "import time\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import argparse\n",
    "from IPython import display\n",
    "import networkx as nx\n",
    "import glob\n",
    "import hashlib\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import plyfile\n",
    "import json\n",
    "from plyfile import PlyData\n",
    "import scipy.io as sio\n",
    "import copy\n",
    "import sys\n",
    "import copy\n",
    "import load_data as loader\n",
    "from utils import Progbar\n",
    "from loss import loss as Loss\n",
    "import utils\n",
    "from scipy.spatial import procrustes\n",
    "from scipy.linalg import expm\n",
    "from models.indiv_crossAttention import crossAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d427d6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "torch.cuda.device_count()\n",
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91a27f4-dc1a-4103-a6f2-24ff96fd5296",
   "metadata": {},
   "outputs": [],
   "source": [
    "T_obs                   = 10\n",
    "T_pred                  = 25\n",
    "T_total                 = T_obs + T_pred\n",
    "batch_size              = 16\n",
    "in_size                 = 90\n",
    "out_size                = 45\n",
    "stochastic_out_size     = out_size * 2\n",
    "hidden_size             = 256\n",
    "embed_size              = 64\n",
    "global dropout_val\n",
    "dropout_val             = 0.2\n",
    "teacher_forcing_ratio   = 0.7\n",
    "avg_n_path_eval         = 20\n",
    "bst_n_path_eval         = 20\n",
    "startpoint_mode         = \"on\"\n",
    "\n",
    "class TrainConfig(object):\n",
    "    \"\"\"Training Configurations\"\"\"\n",
    "    input_window_size = 50  # Input window size during training\n",
    "    output_window_size = 10  # Output window size during training \n",
    "    \n",
    "    hidden_size = 18  \n",
    "    batch_size = 16 \n",
    "    learning_rate = 0.001 \n",
    "    max_epoch = 500\n",
    "    training_size = 200 \n",
    "    validation_size = 20 \n",
    "    restore = False\n",
    "    longterm = False\n",
    "    context_window = 1 \n",
    "    visualize = False\n",
    "    model = 'cmatp'\n",
    "    bone_dim = 3  # dimension of one bone representation, static in all datasets\n",
    "\n",
    "    def __init__(self, dataset, datatype, action, gpu, training, visualize):\n",
    "        self.device_ids = gpu  # index of GPU used to train the model\n",
    "        self.train_model = training  # train or predict\n",
    "        self.visualize = visualize \n",
    "        self.dataset = dataset\n",
    "        self.datatype = datatype\n",
    "        self.filename = action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbf2a37-674b-44a2-ad14-404247337d24",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### CHOOSE DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c16e4a8-9a42-49f3-aa01-c3692e1b85b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetChooser(object):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.dataset = config.dataset #Human\n",
    "\n",
    "    def choose_dataset(self, train=True, prediction=False):\n",
    "        if not prediction:\n",
    "            if self.config.datatype == 'lie' or 'xyz' or 'xyzl' or 'xyzk':\n",
    "                if self.dataset == 'Human':\n",
    "                    bone_length_path = None\n",
    "                    data = loader.HumanDataset(self.config)\n",
    "                    self.config.input_size = data[0]['encoder_inputs'].shape[1]\n",
    "        else:\n",
    "            if self.config.datatype == 'lie' or 'xyz' or 'xyzl' or 'xyzk':\n",
    "                if self.dataset == 'Human':\n",
    "                    bone_length_path = None\n",
    "                    data_loader = loader.HumanPredictionDataset(self.config)\n",
    "                    data = data_loader.get_data()\n",
    "                    self.config.input_size = data[0][list(data[0].keys())[0]].shape[2]\n",
    "                    \n",
    "        bone = np.array([[0., 0., 0.],\n",
    "                         [132.95, 0., 0.],\n",
    "                         [442.89, 0., 0.],\n",
    "                         [454.21, 0., 0.],\n",
    "                         [162.77, 0., 0.],\n",
    "                         [75., 0., 0.],\n",
    "                         [132.95, 0., 0.],\n",
    "                         [442.89, 0., 0.],\n",
    "                         [454.21, 0., 0.],\n",
    "                         [162.77, 0., 0.],\n",
    "                         [75., 0., 0.],\n",
    "                         [0., 0., 0.],\n",
    "                         [233.38, 0., 0.],\n",
    "                         [257.08, 0., 0.],\n",
    "                         [121.13, 0., 0.],\n",
    "                         [115., 0., 0.],\n",
    "                         [257.08, 0., 0.],\n",
    "                         [151.03, 0., 0.],\n",
    "                         [278.88, 0., 0.],\n",
    "                         [251.73, 0., 0.],\n",
    "                         [0., 0., 0.],\n",
    "                         [100., 0., 0.],\n",
    "                         [137.5, 0., 0.],\n",
    "                         [0., 0., 0.],\n",
    "                         [257.08, 0., 0.],\n",
    "                         [151.03, 0., 0.],\n",
    "                         [278.88, 0., 0.],\n",
    "                         [251.73, 0., 0.],\n",
    "                         [0., 0., 0.],\n",
    "                         [100., 0., 0.],\n",
    "                         [137.5, 0., 0.],\n",
    "                         [0., 0., 0.]])\n",
    "\n",
    "        return data, bone\n",
    "\n",
    "    def __call__(self, train=True, prediction=False):\n",
    "        return self.choose_dataset(train, prediction)\n",
    "\n",
    "    def cal_bone_length(self, rawdata):\n",
    "        njoints = rawdata.shape[1]\n",
    "        bone = np.zeros([njoints, 3])\n",
    "        \n",
    "        if self.config.datatype == 'lie':\n",
    "            for i in range(njoints):\n",
    "                bone[i, 0] = round(rawdata[0, i, 3], 2)\n",
    "            bone = bone[1:, :]\n",
    "        elif self.config.datatype == 'xyz' or 'xyzl' or 'xyzk':\n",
    "            for i in range(njoints):\n",
    "                bone[i, 0] = round(np.linalg.norm(rawdata[0, i, :] - rawdata[0, i - 1, :]), 2)\n",
    "\n",
    "        return bone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfeb382-a697-42fa-bfe5-6b660a2c5091",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95340283-5ce7-4c4c-a5b7-f4ea4507b887",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    gpu=[0]\n",
    "    training=True\n",
    "    action='all' #choose one action in the dataset:= ['directions', 'discussion', 'eating', 'greeting', 'phoning', 'posing', 'purchases', 'sitting',\"\"'sittingdown', 'smoking', 'takingphoto', 'waiting', 'walking', 'walkingdog', 'walkingtogether']\"\"'all means all of the above\"\n",
    "    dataset='Human'\n",
    "    datatype='xyz'\n",
    "    visualize=0\n",
    "args=Args()\n",
    "\n",
    "config = TrainConfig(args.dataset, args.datatype, args.action, args.gpu, args.training, args.visualize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec6087c-5c22-4977-8892-91c36b61243b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading Data')\n",
    "\n",
    "choose = DatasetChooser(config)\n",
    "\n",
    "print(\"TRAIN DATASET & TRAIN LOADER\")\n",
    "train_dataset, bone_length = choose(train=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "\n",
    "print(\"TEST DATASET & TEST LOADER\")\n",
    "test_dataset, _ = choose(train=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "\n",
    "print(\"PREDICTION DATASET\")\n",
    "prediction_dataset, bone_length = choose(prediction=True)\n",
    "x_test, y_test, dec_in_test = prediction_dataset\n",
    "\n",
    "print('Final Loading Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e7fb63-8ddb-435e-8d69-7a8b9c61f827",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Kendall Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b09886a-30de-456f-ac3c-d6ff132f4375",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    gpu=[0] \n",
    "    training=True\n",
    "    action='all' \n",
    "    dataset='Human'\n",
    "    datatype='xyzk' #Kendall\n",
    "    visualize=0\n",
    "args=Args()\n",
    "\n",
    "config = TrainConfig(args.dataset, args.datatype, args.action, args.gpu, args.training, args.visualize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54740b3e-9196-4dd3-813b-51ea42245b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inv_exp(X, Y):\n",
    "        \n",
    "    # Check if X and Y have more than one row\n",
    "    if X.shape[0] <= 1 or Y.shape[0] <= 1:\n",
    "        # Handle the case when matrices have one row\n",
    "        pass\n",
    "        return Y\n",
    "\n",
    "    # Apply Procrustes to align Y sur X\n",
    "    _, Y_aligned, _ = procrustes(X, Y)\n",
    "    # Calculate the invExp matrix on aligned matrices after applying Cenetered Scaled function\n",
    "    skeleton = X.dot(Y_aligned.T)\n",
    "    tr = abs(skeleton.trace())\n",
    "    if tr > 1:\n",
    "        tr = 1\n",
    "    teta_invexp = math.acos(tr)\n",
    "    if math.sin(teta_invexp) < 0.0001:\n",
    "        teta_invexp = 0.1\n",
    "\n",
    "    invExp = (teta_invexp / math.sin(teta_invexp)) * (Y_aligned - (math.cos(teta_invexp)) * X)\n",
    "    np_inv = np.array(invExp)\n",
    "\n",
    "    return invExp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7a5a8d-53d9-4f0d-8cc1-3705561398b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading Data')\n",
    "\n",
    "choose = DatasetChooser(config)\n",
    "\n",
    "print(\"TRAIN DATASET & TRAIN LOADER\")\n",
    "train_dataset, bone_length = choose(train=True)\n",
    "\n",
    "ref_skel_train_encoder = copy.deepcopy(train_dataset[0]['encoder_inputs']) \n",
    "ref_skel_train_decoder_inputs = copy.deepcopy(train_dataset[0]['decoder_inputs'])\n",
    "ref_skel_train_decoder_outputs = copy.deepcopy(train_dataset[0]['decoder_outputs'])\n",
    "\n",
    "train_dataset_k = [\n",
    "    {\n",
    "        'encoder_inputs': inv_exp(ref_skel_train_encoder, sample['encoder_inputs']),\n",
    "        'decoder_inputs': inv_exp(ref_skel_train_encoder, sample['decoder_inputs']),\n",
    "        'decoder_outputs': inv_exp(ref_skel_train_decoder_outputs, sample['decoder_outputs'])\n",
    "    }\n",
    "    for sample in train_dataset\n",
    "]\n",
    "\n",
    "train_loader_k = DataLoader(train_dataset_k, batch_size=config.batch_size, shuffle=True)\n",
    "\n",
    "print(\"TEST DATASET & TEST LOADER\")\n",
    "test_dataset, _ = choose(train=False)\n",
    "\n",
    "ref_skel_test_encoder = copy.deepcopy(test_dataset[0]['encoder_inputs'])\n",
    "ref_skel_test_decoder_inputs = copy.deepcopy(test_dataset[0]['decoder_inputs'])\n",
    "ref_skel_test_decoder_outputs = copy.deepcopy(test_dataset[0]['decoder_outputs'])\n",
    "\n",
    "test_dataset_k= [\n",
    "    {\n",
    "        'encoder_inputs': inv_exp(ref_skel_test_encoder, sample['encoder_inputs']),\n",
    "        'decoder_inputs': inv_exp(ref_skel_test_encoder, sample['decoder_inputs']),\n",
    "        'decoder_outputs': inv_exp(ref_skel_test_decoder_outputs, sample['decoder_outputs'])\n",
    "    }\n",
    "    for sample in test_dataset\n",
    "]\n",
    "\n",
    "test_loader_k = DataLoader(test_dataset_k, batch_size=config.batch_size, shuffle=True)\n",
    "\n",
    "print('Final Loading Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c89705-9836-4a5f-893e-5d3731e05d45",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### LIE PROCESS DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0168478d-e80d-48ed-853f-14655da8a9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    gpu=[0]\n",
    "    training=True\n",
    "    action='all' \n",
    "    dataset='Human'\n",
    "    datatype='xyzl' #lie process\n",
    "    visualize=0\n",
    "args=Args()\n",
    "\n",
    "config = TrainConfig(args.dataset, args.datatype, args.action, args.gpu, args.training, args.visualize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd579bf-df93-4b1d-bce4-f7f6553fc86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_global_transformation(skeleton, ref_skeleton):\n",
    "    # Calculate global rotation\n",
    "    rotation_matrix = np.dot(skeleton, np.transpose(ref_skeleton))\n",
    "    u, s, v = np.linalg.svd(rotation_matrix, full_matrices=False)\n",
    "    rotation_matrix = np.dot(v.T, u.T)\n",
    "    # Calculate global translation\n",
    "    translation_vector = np.mean(ref_skeleton, axis=1) - np.dot(rotation_matrix, np.mean(skeleton, axis=1))\n",
    "    return rotation_matrix, translation_vector\n",
    "\n",
    "def to_SE3(rotation_matrix, translation_vector):\n",
    "    se3_matrix = np.eye(4)\n",
    "    se3_matrix[:3, :3] = rotation_matrix[:3, :3]  # Take the top-left 3x3 block\n",
    "    se3_matrix[:3, 3] = translation_vector[:3]  # Take the first 3 elements\n",
    "    return se3_matrix\n",
    "\n",
    "def extract_point_in_SE3(se3_matrix):\n",
    "    return se3_matrix\n",
    "\n",
    "def derive_tangent_space(rotation_matrix, translation_vector):\n",
    "    # Ensure rotation_matrix is 3x3\n",
    "    rotation_matrix = rotation_matrix[:3, :3]\n",
    "    # Create a 3x3 identity matrix\n",
    "    identity_matrix = np.eye(3)\n",
    "    # Ensure translation_vector is a column vector\n",
    "    translation_vector = translation_vector[:3].reshape(-1, 1)\n",
    "    # Calculate the skew-symmetric matrix directly\n",
    "    skew_symmetric_matrix = rotation_matrix - identity_matrix\n",
    "    skew_symmetric_matrix_flat = skew_symmetric_matrix.flatten()\n",
    "    tangent_space = np.zeros((4, 4))\n",
    "    tangent_space[:3, :3] = rotation_matrix\n",
    "    tangent_space[:3, 3] = translation_vector.flatten()\n",
    "    tangent_space[3, :3] = skew_symmetric_matrix_flat[:3] \n",
    "\n",
    "    return tangent_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b26f14-9e71-4729-b2cb-d30d153cb827",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lie_group_and_algebra_transform_s(skeleton, ref_skeleton):\n",
    "        \n",
    "    skeleton = skeleton.cpu().numpy() if isinstance(skeleton, torch.Tensor) else skeleton\n",
    "    ref_skeleton = ref_skeleton.cpu().numpy() if isinstance(ref_skeleton, torch.Tensor) else ref_skeleton\n",
    "    skeleton = skeleton.reshape(-1, 90) \n",
    "    # Calculate global transformation (rotation and translation)\n",
    "    rotation_matrix, translation_vector = calculate_global_transformation(skeleton, ref_skeleton)\n",
    "    se3_matrix = to_SE3(rotation_matrix, translation_vector)\n",
    "    # Extract a representative point in Lie group (SE(3))\n",
    "    point_in_SE3 = extract_point_in_SE3(se3_matrix)\n",
    "    # Derive tangent space (Lie algebra) associated with SE(3)\n",
    "    tangent_space = derive_tangent_space(rotation_matrix, translation_vector)\n",
    "    return tangent_space #point_in_SE3, tangent_space\n",
    "\n",
    "def lie_group_and_algebra_transform(frames, ref_skeleton):\n",
    "    result = np.array([lie_group_and_algebra_transform_s(skeleton, ref_skeleton) for skeleton in frames])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d48e1b-0873-46bb-8516-868be5d1487f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading Data')\n",
    "\n",
    "choose = DatasetChooser(config)\n",
    "\n",
    "print(\"TRAIN DATASET & TRAIN LOADER\")\n",
    "train_dataset, bone_length = choose(train=True)\n",
    "\n",
    "ref_skel_train_encoder = copy.deepcopy(train_dataset[0]['encoder_inputs'])\n",
    "\n",
    "# Apply Lie group and Lie algebra transformations to the data\n",
    "train_dataset_l = [\n",
    "    {\n",
    "        'encoder_inputs': lie_group_and_algebra_transform(sample['encoder_inputs'],ref_skel_train_encoder),\n",
    "        'decoder_inputs': lie_group_and_algebra_transform(sample['decoder_inputs'],ref_skel_train_decoder_inputs),\n",
    "        'decoder_outputs': lie_group_and_algebra_transform(sample['decoder_outputs'],ref_skel_train_decoder_outputs)\n",
    "    }\n",
    "    for sample in train_dataset\n",
    "]\n",
    "\n",
    "train_loader_l = DataLoader(train_dataset_l, batch_size=config.batch_size, shuffle=True)\n",
    "\n",
    "print(\"TEST DATASET & TEST LOADER\")\n",
    "test_dataset, _ = choose(train=False)\n",
    "\n",
    "ref_skel_test_encoder = copy.deepcopy(test_dataset[0]['encoder_inputs'])\n",
    "ref_skel_test_decoder_inputs = copy.deepcopy(test_dataset[0]['decoder_inputs'])\n",
    "ref_skel_test_decoder_outputs = copy.deepcopy(test_dataset[0]['decoder_outputs'])\n",
    "\n",
    "test_dataset_l= [\n",
    "    {\n",
    "        'encoder_inputs': lie_group_and_algebra_transform(sample['encoder_inputs'],ref_skel_test_encoder),\n",
    "        'decoder_inputs': lie_group_and_algebra_transform(sample['decoder_inputs'],ref_skel_test_decoder_inputs),\n",
    "        'decoder_outputs': lie_group_and_algebra_transform(sample['decoder_outputs'],ref_skel_test_decoder_outputs)\n",
    "    }\n",
    "    for sample in test_dataset\n",
    "]\n",
    "\n",
    "test_loader_l = DataLoader(test_dataset_l, batch_size=config.batch_size, shuffle=True)\n",
    "\n",
    "print('Final Loading Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dffe457-105c-461c-a6eb-bc1bcbe397d1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e048b8-34b7-4fc5-b493-a98e7814a7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def position_embedding(input, d_model):\n",
    "    input = input.view(-1, 1)\n",
    "    dim = torch.arange(d_model // 2, dtype=torch.float32, device=input.device).view(1, -1)\n",
    "    sin = torch.sin(input / 10000 ** (2 * dim / d_model))\n",
    "    cos = torch.cos(input / 10000 ** (2 * dim / d_model))\n",
    "\n",
    "    out = torch.zeros((input.shape[0], d_model), device=input.device)\n",
    "    out[:, ::2] = sin\n",
    "    out[:, 1::2] = cos\n",
    "    return out\n",
    "\n",
    "def sinusoid_encoding_table(max_len, d_model):\n",
    "    pos = torch.arange(max_len, dtype=torch.float32)\n",
    "    out = position_embedding(pos, d_model)\n",
    "    return out\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, d_model, d_k, d_v, h):\n",
    "        \"\"\"\n",
    "        param:\n",
    "        d_model: Output dimensionality of the model\n",
    "        d_k: Dimensionality of queries and keys\n",
    "        d_v: Dimensionality of values\n",
    "        h: Number of heads\n",
    "        \"\"\"\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.fc_q = nn.Linear(d_model, h * d_k)\n",
    "        self.fc_k = nn.Linear(d_model, h * d_k)\n",
    "        self.fc_v = nn.Linear(d_model, h * d_v)\n",
    "        self.fc_o = nn.Linear(h * d_v, d_model)\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.h = h\n",
    "\n",
    "        self.init_weights(gain=1.0)\n",
    "\n",
    "    def init_weights(self, gain=1.0):\n",
    "        nn.init.xavier_normal_(self.fc_q.weight, gain=gain)\n",
    "        nn.init.xavier_normal_(self.fc_k.weight, gain=gain)\n",
    "        nn.init.xavier_normal_(self.fc_v.weight, gain=gain)\n",
    "        nn.init.xavier_normal_(self.fc_o.weight, gain=gain)\n",
    "        nn.init.constant_(self.fc_q.bias, 0)\n",
    "        nn.init.constant_(self.fc_k.bias, 0)\n",
    "        nn.init.constant_(self.fc_v.bias, 0)\n",
    "        nn.init.constant_(self.fc_o.bias, 0)\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "        \"\"\"\n",
    "        Computes\n",
    "        :param queries: Queries (b_s, nq, d_model)\n",
    "        :param keys: Keys (b_s, nk, d_model)\n",
    "        :param values: Values (b_s, nk, d_model)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        b_s, nq = queries.shape[:2]\n",
    "        nk = keys.shape[1]\n",
    "        q = self.fc_q(queries).view(b_s, nq, self.h, self.d_k).permute(0, 2, 1, 3)  # (b_s, h, nq, d_k)\n",
    "        k = self.fc_k(keys).view(b_s, nk, self.h, self.d_k).permute(0, 2, 3, 1)  # (b_s, h, d_k, nk)\n",
    "        v = self.fc_v(values).view(b_s, nk, self.h, self.d_v).permute(0, 2, 1, 3)  # (b_s, h, nk, d_v)\n",
    "\n",
    "        att = torch.matmul(q, k) / np.sqrt(self.d_k)  # (b_s, h, nq, nk)\n",
    "\n",
    "        att = torch.softmax(att, -1)\n",
    "\n",
    "        out = torch.matmul(att, v).permute(0, 2, 1, 3).contiguous().view(b_s, nq, self.h * self.d_v)  # (b_s, nq, h*d_v)\n",
    "        out = self.fc_o(out)  # (b_s, nq, d_model)\n",
    "        return out\n",
    "    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, d_k, d_v, h, dff=2048, dropout=.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.attention = ScaledDotProductAttention(d_model=d_model, d_k=d_k, d_v=d_v, h=h)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "        self.fc = nn.Sequential(*[nn.Linear(d_model, dff), nn.ReLU(inplace=True), nn.Dropout(p=dropout),nn.Linear(dff, d_model)])\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "        att = self.attention(queries, keys, values)\n",
    "        att = self.dropout(att)\n",
    "        att = self.fc(att)\n",
    "        att = self.dropout(att)\n",
    "        return self.layer_norm(queries + att)\n",
    "    \n",
    "class EncoderSelfAttention(nn.Module):\n",
    "    def __init__(self, device, d_model, d_k, d_v, n_head, dff=2048, dropout_transformer=.1, n_module=6):\n",
    "        super(EncoderSelfAttention, self).__init__()\n",
    "        self.encoder = nn.ModuleList([MultiHeadAttention(d_model, d_k, d_v, n_head, dff, dropout_transformer) for _ in range(n_module)])\n",
    "        self.device = device\n",
    "    \n",
    "    def forward(self, x): \n",
    "        in_encoder = x + sinusoid_encoding_table(x.shape[1], x.shape[2]).expand(x.shape).to(self.device)\n",
    "        for l in self.encoder:\n",
    "            in_encoder = l(in_encoder, in_encoder, in_encoder)\n",
    "            \n",
    "        return in_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38ec1b3-dcd5-4c5e-b0c1-a0d415457110",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.query = nn.Linear(input_dim, hidden_dim)\n",
    "        self.key = nn.Linear(input_dim, hidden_dim)\n",
    "        self.value = nn.Linear(input_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "\n",
    "        attention_scores = F.softmax(torch.matmul(q, k.transpose(-2, -1)) / (x.size(-1) ** 0.5), dim=-1)\n",
    "        output = torch.matmul(attention_scores, v)\n",
    "\n",
    "        return output\n",
    "\n",
    "class CoordinatesTransformer_k(nn.Module):\n",
    "    def __init__(self, device, dropout1d,input_dim=90, hidden_size=256, output_dim=256):\n",
    "        super(CoordinatesTransformer_k, self).__init__()\n",
    "        self.device = device\n",
    "        self.dropout1d = dropout1d\n",
    "\n",
    "        # Self-Attention Layer\n",
    "        self.self_attention = SelfAttention(input_dim, hidden_size)\n",
    "\n",
    "        # Feedforward Layers\n",
    "        self.linear1 = nn.Linear(hidden_size, hidden_size)  # Adjusted input_dim to hidden_size\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout1d)\n",
    "        self.linear2 = nn.Linear(hidden_size, output_dim)\n",
    "\n",
    "    def forward(self, input_tensor_k):\n",
    "        # Assuming input_tensor_k shape: [batch_size, sequence_length, input_dim]\n",
    "        \n",
    "        # Self-Attention\n",
    "        self_attended = self.self_attention(input_tensor_k)\n",
    "        # Feedforward Layers\n",
    "        output = self.linear1(self_attended)\n",
    "        output = self.relu(output)\n",
    "        output = self.dropout(output)\n",
    "        output = self.linear2(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f22201-0a37-4c5c-a364-9399413bcae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.query = nn.Linear(input_dim, hidden_dim)\n",
    "        self.key = nn.Linear(input_dim, hidden_dim)\n",
    "        self.value = nn.Linear(input_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "        attention_scores = F.softmax(torch.matmul(q, k.transpose(-2, -1)) / (x.size(-1) ** 0.5), dim=-1)\n",
    "        output = torch.matmul(attention_scores, v)\n",
    "\n",
    "        return output\n",
    "\n",
    "class CoordinatesTransformer_l(nn.Module):\n",
    "    def __init__(self, device, dropout1d, input_dim=16, hidden_size=256, output_dim=256):\n",
    "        super(CoordinatesTransformer_l, self).__init__()\n",
    "        self.device = device\n",
    "        self.dropout1d = dropout1d\n",
    "        # Self-Attention Layer\n",
    "        self.self_attention = SelfAttention(input_dim, hidden_size)\n",
    "        # Feedforward Layers\n",
    "        self.linear1 = nn.Linear(hidden_size, hidden_size)  # Adjusted input_dim to hidden_size\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout1d)\n",
    "        self.linear2 = nn.Linear(hidden_size, output_dim)\n",
    "\n",
    "    def forward(self, input_tensor_l):\n",
    "        # Assuming input_tensor_k shape: [batch_size, sequence_length, input_dim]\n",
    "       \n",
    "        # Self-Attention\n",
    "        self_attended = self.self_attention(input_tensor_l)\n",
    "        # Feedforward Layers\n",
    "        output = self.linear1(self_attended)\n",
    "        output = self.relu(output)\n",
    "        output = self.dropout(output)\n",
    "        output = self.linear2(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84acf023-828b-433a-8cb8-85f0d0ee1345",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead)\n",
    "        self.norm1 = nn.LayerNorm(d_model) \n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.feed_forward = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x, e_output):\n",
    "        # Multihead self-attention\n",
    "        attn_output, _ = self.self_attn(x, x, x)\n",
    "        x = x + self.norm1(attn_output)\n",
    "        # Feed forward\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = x + self.norm2(ff_output)\n",
    "        return x\n",
    "\n",
    "    \n",
    "class DecoderTransformer(nn.Module):\n",
    "    def __init__(self, in_size, embed_size, hidden_size, d_model=512, dropout_val=dropout_val, batch_size=1, nhead=8, num_layers=6):\n",
    "        super(DecoderTransformer, self).__init__()\n",
    "                \n",
    "        self.in_size                = in_size \n",
    "        self.stochastic_out_size    = stochastic_out_size\n",
    "        self.hidden_size            = hidden_size \n",
    "        self.batch_size             = batch_size\n",
    "        self.embed_size             = embed_size\n",
    "        self.seq_length             = T_pred\n",
    "        self.dropout_val            = dropout_val\n",
    "        self.d_model                = d_model\n",
    "        self.nhead                  = nhead\n",
    "        self.num_layers             = num_layers\n",
    "        \n",
    "        self.embedder_rho = nn.Linear(90, 200)\n",
    "        self.fC_mu = nn.Sequential(nn.Linear(self.hidden_size + self.hidden_size + 2, int(self.hidden_size/2), bias=True),nn.ReLU(),nn.Dropout(p=dropout_val),nn.Linear(int(self.hidden_size/2), self.stochastic_out_size, bias=True))\n",
    "        self.dropout = nn.Dropout(dropout_val)\n",
    "        self.reducted_size = int((self.hidden_size-1)/3)+1\n",
    "        self.reducted_size2 = int((self.hidden_size+in_size-1)/3)+1\n",
    "        self.FC_dim_red = nn.Sequential(nn.MaxPool2d(kernel_size=3, stride=3, padding=1),nn.Flatten(start_dim=1, end_dim=-1),nn.Linear(self.reducted_size*self.reducted_size2, 2*self.hidden_size+in_size, bias=True),nn.ReLU())\n",
    "        \n",
    "        self.embedding = nn.Linear(200, d_model)\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model, nhead) for _ in range(num_layers)])\n",
    "        self.output_layer = nn.Linear(d_model, 514)\n",
    "                \n",
    "    def forward(self, x, encoder_outputs):   \n",
    "        \n",
    "        # Coordination Embedding\n",
    "        embedding = self.embedder_rho(x.view(x.shape[0], 1, -1))\n",
    "        embedding = F.relu(self.dropout(embedding))\n",
    "        # Embed the decoder input\n",
    "        x = self.embedding(embedding)\n",
    "        for layer in self.layers: #6 layers\n",
    "            # Move the layer to the same device as `x`\n",
    "            # Perform the forward pass\n",
    "            x = layer(x, encoder_outputs)\n",
    "            if torch.isnan(x).any():\n",
    "                print(\"NaN values found in x layer\")\n",
    "            if torch.isinf(x).any():\n",
    "                print(\"infinite values found in x layer\")\n",
    "    \n",
    "        output = self.output_layer(x)\n",
    "        prediction = self.fC_mu(output.squeeze(0)) \n",
    "        \n",
    "        return prediction\n",
    "    \n",
    "    def dim_red(self, input):\n",
    "        output = self.FC_dim_red(input)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3322d5e6-bd5b-4036-89fe-87c99c11fc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, in_size, embed_size, hidden_size, batch_size, d_model=512, d_ff=2048, h=8, dropout_val=dropout_val, N=6, input_dim=512):\n",
    "        super(Model, self).__init__()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        self.encoder_k = CoordinatesTransformer_k(device,dropout1d=dropout_val)\n",
    "        self.encoder_k.apply(init_weights)\n",
    "        self.encoder_l = CoordinatesTransformer_l(device,dropout1d=dropout_val)\n",
    "        self.encoder_l.apply(init_weights)\n",
    "        self.decoder = DecoderTransformer(in_size, embed_size, hidden_size, num_layers=6, nhead=8)\n",
    "        self.decoder.apply(init_weights)\n",
    "        self.crossAttention = crossAttention(N=6,d_model=256, d_ff=2048, h=8, dropout=0.1)\n",
    "                    \n",
    "        if device.type=='cuda':\n",
    "            self.encoder_k.cuda()\n",
    "            self.encoder_l.cuda()\n",
    "            self.decoder.cuda()\n",
    "\n",
    "    def forward(self, input_tensor_k, input_tensor_l, input_tensor, output_tensor, batch_size, train_mode):       \n",
    "        \n",
    "        batch_size      = int(input_tensor_k.size(0))\n",
    "        encoder_outputs = torch.zeros(batch_size, config.input_window_size, hidden_size).cuda()\n",
    "        start_point = (input_tensor[:,0,:]).to(device).clone().detach()\n",
    "        if startpoint_mode==\"on\":\n",
    "            input_tensor[:,0,:]    = 0\n",
    "            \n",
    "        input_tensor_l = input_tensor_l.reshape(input_tensor_l.size(0), input_tensor_l.size(1), input_tensor_l.size(-1) * input_tensor_l.size(-2))\n",
    "        encoder_outputs_k = self.encoder_k(input_tensor_k)\n",
    "        encoder_outputs_l = self.encoder_l(input_tensor_l)\n",
    "        \n",
    "        src_mask = None\n",
    "        obd_enc_mask = None\n",
    "        cross_ouput = self.crossAttention( encoder_outputs_k, encoder_outputs_l, src_mask, obd_enc_mask)\n",
    "        e_outputs=cross_ouput\n",
    "        \n",
    "        decoder_input = input_tensor[:,-1,:] \n",
    "        outputs                         = torch.zeros(batch_size, T_pred , in_size).cuda() \n",
    "        stochastic_outputs              = torch.zeros(batch_size, T_pred , stochastic_out_size).cuda()\n",
    "        teacher_force                   = 1\n",
    "        epsilonX                        = Normal(torch.zeros(batch_size,1),torch.ones(batch_size,1))\n",
    "        teacher_force                   = int(random.random() < teacher_forcing_ratio) if train_mode else 0\n",
    "    \n",
    "        for t in range(0, config.output_window_size-1):\n",
    "            output = self.decoder(decoder_input, e_outputs)\n",
    "            # Reparameterization Trick :)\n",
    "            decoder_output              = torch.zeros(batch_size,1,90).cuda()\n",
    "            for i in range(0,out_size):\n",
    "                epsilonX               = Normal(torch.zeros(batch_size,1),torch.ones(batch_size,1))\n",
    "                decoder_output[:,:,i]  = output[:,:,2*i] + epsilonX.sample((avg_n_path_eval,1)).view(-1,avg_n_path_eval,1).mean(-2).cuda() * output[:,:,2*i+1]\n",
    "\n",
    "            outputs[:,t,:]                        = decoder_output.squeeze(1)\n",
    "            stochastic_outputs[:,t,:]             = output.squeeze(1)\n",
    "            predictionns = outputs\n",
    "                                            \n",
    "        return predictionns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4be4d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kendall transformation and mapping to tangent space:\n",
    "\n",
    "def CenteredScaledd(X):\n",
    "    # Convert to PyTorch tensor\n",
    "    X_reshaped = torch.tensor(X)\n",
    "    # Reshape the input tensor to (n_frames, n_joints, k_dimensions)\n",
    "    n_frames, total_dimensions = X_reshaped.shape\n",
    "    n_joints = total_dimensions // 3\n",
    "    k_dimensions = 3\n",
    "    X_reshaped = X_reshaped.view((n_frames, n_joints, k_dimensions))\n",
    "    # Centering: Subtract the mean of each joint across all frames\n",
    "    X_reshaped = X_reshaped - torch.mean(X_reshaped, dim=0)\n",
    "    # Calculate the \"centered\" Frobenius norm for each joint\n",
    "    normX = torch.norm(X_reshaped, dim=(1, 2), p='fro')\n",
    "    # Scale to equal (unit) norm for each joint\n",
    "    X_reshaped = X_reshaped / normX[:, None, None]\n",
    "    # Reshape back to the original shape\n",
    "    X_scaled = X_reshaped.view((n_frames, total_dimensions))\n",
    "    return X_scaled\n",
    "\n",
    "def inv_exp(X, Y):\n",
    "    if isinstance(X, torch.Tensor):\n",
    "        X = X.cpu().numpy()\n",
    "    if isinstance(Y, torch.Tensor):\n",
    "        Y = Y.cpu().numpy()\n",
    "                \n",
    "    # Check if X and Y have more than one row\n",
    "    if X.shape[0] <= 1 or Y.shape[0] <= 1:\n",
    "        # Handle the case when matrices have one row\n",
    "        pass\n",
    "        return Y\n",
    "\n",
    "    _, Y_aligned, _ = procrustes(X, Y)\n",
    "    skeleton = X.dot(Y_aligned.T)\n",
    "    tr = abs(skeleton.trace())\n",
    "    if tr > 1:\n",
    "        tr = 1\n",
    "    teta_invexp = math.acos(tr)\n",
    "    if math.sin(teta_invexp) < 0.0001:\n",
    "        teta_invexp = 0.1\n",
    "    invExp = (teta_invexp / math.sin(teta_invexp)) * (Y_aligned - (math.cos(teta_invexp)) * X)\n",
    "    np_inv = np.array(invExp)\n",
    "    return invExp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aedc445-68a5-4f1f-980c-c422ca5bb136",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a62f8e4-615d-4c7b-9548-f2e761ef7494",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_net(config):\n",
    "    if config.model == 'ST_HRN':\n",
    "        net = ST_HRN(config)\n",
    "    elif config.model == 'HMR':\n",
    "        net = HMR(config)\n",
    "    elif config.model == 'cmatp': #yes\n",
    "        net = Model(in_size, embed_size, hidden_size, dropout_val=dropout_val, batch_size=batch_size)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8561f3b3-96d5-46f4-9e71-7531b9321967",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config, checkpoint_dir):\n",
    "    \n",
    "    print('Start Training the Model!')\n",
    "\n",
    "    # generate data loader\n",
    "    if config.longterm is True:\n",
    "        config.output_window_size = 100\n",
    "        \n",
    "    choose = DatasetChooser(config)\n",
    "    train_dataset, bone_length = choose(train=True)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "    test_dataset, _ = choose(train=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "    prediction_dataset, bone_length = choose(prediction=True)\n",
    "    x_test, y_test, dec_in_test = prediction_dataset\n",
    "    \n",
    "    device = torch.device(\"cuda:\"+str(config.device_ids[0]) if torch.cuda.is_available() else \"cpu\")\n",
    "    print('Device {} will be used to save parameters'.format(device)) #cuda\n",
    "    \n",
    "    net = choose_net(config)\n",
    "    net.to(device)\n",
    "\n",
    "    if config.restore is True:\n",
    "        dir = utils.get_file_list(checkpoint_dir)\n",
    "        print('Load model from:' + checkpoint_dir + dir[-1])\n",
    "        net.load_state_dict(torch.load(checkpoint_dir + dir[-1], map_location='cuda:0'))\n",
    "\n",
    "    optimizer = optim.Adam(net.parameters(), lr=config.learning_rate)\n",
    "    \n",
    "    if not (os.path.exists(checkpoint_dir)):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    print('Checkpoint dir:', checkpoint_dir)\n",
    "\n",
    "    best_error = float('inf')\n",
    "    best_error_list = None\n",
    "    for epoch in range(config.max_epoch):\n",
    "        print(\"At epoch:{}\".format(str(epoch + 1)))\n",
    "        prog = Progbar(target=config.training_size)\n",
    "        prog_valid = Progbar(target=config.validation_size)\n",
    "\n",
    "        # Train\n",
    "        for it in range(config.training_size):\n",
    "            for i, (data, data_k, data_l) in enumerate(zip(train_loader, train_loader_k, train_loader_l), 0):\n",
    "                \n",
    "                encoder_inputs = data['encoder_inputs'].float().to(device)\n",
    "                decoder_inputs = data['decoder_inputs'].float().to(device)\n",
    "                decoder_outputs = data['decoder_outputs'].float().to(device)\n",
    "                          \n",
    "                encoder_inputs_k = data_k['encoder_inputs'].float().to(device)\n",
    "                decoder_inputs_k = data_k['decoder_inputs'].float().to(device)\n",
    "                decoder_outputs_k = data_k['decoder_outputs'].float().to(device)\n",
    "                \n",
    "                encoder_inputs_l = data_l['encoder_inputs'].float().to(device)\n",
    "                decoder_inputs_l = data_l['decoder_inputs'].float().to(device)\n",
    "                decoder_outputs_l = data_l['decoder_outputs'].float().to(device)\n",
    "\n",
    "                prediction= net(encoder_inputs_k, encoder_inputs_l, encoder_inputs, decoder_outputs, batch_size, train_mode=True)\n",
    "                loss = Loss(prediction, decoder_outputs, bone_length, config)\n",
    "                \n",
    "                net.zero_grad()\n",
    "                loss.backward()\n",
    "                _ = torch.nn.utils.clip_grad_norm_(net.parameters(), 1)\n",
    "                optimizer.step()\n",
    "    \n",
    "            prog.update(it + 1, [(\"Training Loss\", loss.item())])\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for it in range(config.validation_size):\n",
    "                for j in range(3):\n",
    "                    for i, (data, data_k, data_l) in enumerate(zip(train_loader, train_loader_k, train_loader_l), 0):\n",
    "                        if j == 0 and i == 0:\n",
    "                            encoder_inputs = data['encoder_inputs'].float().to(device)\n",
    "                            decoder_inputs = data['decoder_inputs'].float().to(device)\n",
    "                            decoder_outputs = data['decoder_outputs'].float().to(device)\n",
    "                            \n",
    "                            encoder_inputs_k = data_k['encoder_inputs'].float().to(device)\n",
    "                            decoder_inputs_k = data_k['decoder_inputs'].float().to(device)\n",
    "                            decoder_outputs_k = data_k['decoder_outputs'].float().to(device)\n",
    "                            \n",
    "                            encoder_inputs_l = data_l['encoder_inputs'].float().to(device)\n",
    "                            decoder_inputs_l = data_l['decoder_inputs'].float().to(device)\n",
    "                            decoder_outputs_l = data_l['decoder_outputs'].float().to(device)\n",
    "                            \n",
    "                        else:\n",
    "                            encoder_inputs = torch.cat([data['encoder_inputs'].float().to(device), encoder_inputs], dim=0)\n",
    "                            decoder_inputs = torch.cat([data['decoder_inputs'].float().to(device), decoder_inputs], dim=0)\n",
    "                            decoder_outputs = torch.cat([data['decoder_outputs'].float().to(device), decoder_outputs], dim=0)\n",
    "\n",
    "                            encoder_inputs_k = torch.cat([data_k['encoder_inputs'].float().to(device), encoder_inputs_k], dim=0)\n",
    "                            decoder_inputs_k = torch.cat([data_k['decoder_inputs'].float().to(device), decoder_inputs_k], dim=0)\n",
    "                            decoder_outputs_k = torch.cat([data_k['decoder_outputs'].float().to(device), decoder_outputs_k], dim=0)\n",
    "                            \n",
    "                            encoder_inputs_l = torch.cat([data_l['encoder_inputs'].float().to(device), encoder_inputs_l], dim=0)\n",
    "                            decoder_inputs_l = torch.cat([data_l['decoder_inputs'].float().to(device), decoder_inputs_l], dim=0)\n",
    "                            decoder_outputs_l = torch.cat([data_l['decoder_outputs'].float().to(device), decoder_outputs_l], dim=0)\n",
    "                            \n",
    "                prediction= net(encoder_inputs_k, encoder_inputs_l, encoder_inputs, decoder_outputs, batch_size, train_mode=False)\n",
    "                loss = Loss(prediction, decoder_outputs, bone_length, config)\n",
    "                prog_valid.update(it+1, [(\"Testing Loss\", loss.item())])\n",
    "                \n",
    "        \n",
    "        #Test prediction\n",
    "        actions = list(x_test.keys())\n",
    "        y_predict = {}\n",
    "        x_testk = {}\n",
    "        x_testl = {}\n",
    "        with torch.no_grad():\n",
    "            for act in actions:\n",
    "                #print(\"actions\", actions) #['directions', 'discussion', 'eating', 'greeting', 'phoning', 'posing', 'purchases', 'sitting', 'sittingdown', 'smoking', 'takingphoto', 'waiting', 'walking', 'walkingdog', 'walkingtogether']\n",
    "                \n",
    "                x_test_ = torch.from_numpy(x_test[act]).float().to(device)\n",
    "                ref_pred_xtest = copy.deepcopy(prediction_dataset[0][act][0])\n",
    "                \n",
    "                x_test_batch = torch.from_numpy(x_test[act]).float().to(device)\n",
    "                x_testl[act] = [lie_group_and_algebra_transform(frame,ref_pred_xtest) for frame in x_test_batch]\n",
    "                x_test_l = torch.tensor(x_testl[act], dtype=torch.float32, device=device)\n",
    "                \n",
    "                x_test_batch = torch.from_numpy(x_test[act]).float().to(device)\n",
    "                x_testk[act] = [CenteredScaledd(frame) for frame in x_test_batch]\n",
    "                x_testk[act] = [inv_exp(ref_pred_xtest, frame) for frame in x_testk[act]]\n",
    "                x_test_k = torch.from_numpy(np.array(x_testk[act])).float().to(device)                          \n",
    "                \n",
    "                dec_in_test_ = torch.from_numpy(dec_in_test[act]).float().to(device)      \n",
    "    \n",
    "                pred = net(x_test_k, x_test_l, x_test_, dec_in_test_, batch_size, train_mode=True)\n",
    "    \n",
    "                # Assuming pred is a tensor or array\n",
    "                if isinstance(pred, torch.Tensor) or isinstance(pred, np.ndarray):\n",
    "                    pred = pred.cpu().numpy() # Convert tensor to NumPy array\n",
    "                else:\n",
    "                    print(\"pred is not a valid tensor or array.\")\n",
    "    \n",
    "                y_predict[act] = pred\n",
    "\n",
    "        error_actions = 0.0\n",
    "        for act in actions:\n",
    "            if config.datatype == 'lie' or 'xyz':\n",
    "                mean_error, _ = mean_per_joint_position_error(config, act, y_predict[act], y_test[act][:, :config.output_window_size, :])\n",
    "                error = mean_error[[1, 3, 7, 9]]\n",
    "            error_actions += error.mean()\n",
    "        error_actions /= len(actions)\n",
    "        if error_actions < best_error:\n",
    "            print(error_actions)\n",
    "            print(best_error)\n",
    "            best_error_list = error\n",
    "            best_error = error_actions            \n",
    "            torch.save(net.state_dict(), checkpoint_dir + 'Epoch_' + str(epoch + 1) + '.pth')\n",
    "        print('Current best:' + str(round(best_error_list[0], 2))+ ' ' + str(round(best_error_list[1], 2)) + ' ' + str(round(best_error_list[2], 2)) + ' ' + str(round(best_error_list[3], 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4168035-0011-4083-99bf-9657bfdf97af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_list(file_path):\n",
    "    dir_list = os.listdir(file_path)\n",
    "    if not dir_list:\n",
    "        return\n",
    "    else:\n",
    "        dir_list = sorted(dir_list, key=lambda x: os.path.getmtime(os.path.join(file_path, x)))\n",
    "        return dir_list\n",
    "\n",
    "def mean_per_joint_position_error(config, action, y_predict, y_test):\n",
    "    n_batch = y_predict.shape[0]\n",
    "    nframes = y_predict.shape[1]\n",
    "\n",
    "    mean_errors = np.zeros([n_batch, nframes])\n",
    "    \n",
    "    for i in range(n_batch):\n",
    "        for j in range(nframes):\n",
    "            if config.dataset == 'Human':\n",
    "                pred = unNormalizeData(y_predict[i], config.data_mean, config.data_std, config.dim_to_ignore)\n",
    "                gt = unNormalizeData(y_test[i], config.data_mean, config.data_std, config.dim_to_ignore)\n",
    "            else:\n",
    "                pred = np.copy(y_predict[i])\n",
    "                gt = np.copy(y_test[i])\n",
    "        \n",
    "        # Calculate Euclidean error for each joint position\n",
    "        euc_error = np.linalg.norm(gt - pred, axis=2)\n",
    "        mean_errors[i, :] = np.mean(euc_error, axis=1)\n",
    "\n",
    "    # Mean of errors across batches\n",
    "    mpjpe = np.mean(mean_errors, axis=0)\n",
    "\n",
    "    print(\"\\n\" + action)\n",
    "    toprint_idx = np.array([1, 3, 7, 9, 13, 15, 17, 24])\n",
    "    idx = np.where(toprint_idx < len(mpjpe))[0]\n",
    "    toprint_list = [\"& {:.3f} \".format(mpjpe[toprint_idx[i]]) for i in idx]\n",
    "    print(\"\".join(toprint_list))\n",
    "    mpjpe_mean = np.mean(mpjpe[toprint_idx[idx]])\n",
    "\n",
    "    return mpjpe, mpjpe_mean\n",
    "\n",
    "def unNormalizeData(normalizedData, data_mean, data_std, dimensions_to_ignore):\n",
    "    \"\"\"\n",
    "    Copied from https://github.com/una-dinosauria/human-motion-prediction\n",
    "    \"\"\"\n",
    "\n",
    "    T = normalizedData.shape[0]\n",
    "    D = data_mean.shape[0]\n",
    "\n",
    "    origData = np.zeros((T, D), dtype=np.float32)\n",
    "    dimensions_to_use = []\n",
    "    for i in range(D):\n",
    "        if i in dimensions_to_ignore:\n",
    "            continue\n",
    "        dimensions_to_use.append(i)\n",
    "    dimensions_to_use = np.array(dimensions_to_use)\n",
    "\n",
    "    origData[:, dimensions_to_use] = normalizedData\n",
    "\n",
    "    stdMat = data_std.reshape((1, D))\n",
    "    stdMat = np.repeat(stdMat, T, axis=0)\n",
    "    meanMat = data_mean.reshape((1, D))\n",
    "    meanMat = np.repeat(meanMat, T, axis=0)\n",
    "    origData = np.multiply(origData, stdMat) + meanMat\n",
    "    return origData\n",
    "\n",
    "def rotmat2expmap(R):\n",
    "    theta = np.arccos((np.trace(R) - 1) / 2.0)\n",
    "    if theta < 1e-6:\n",
    "        A = np.zeros((3, 1))\n",
    "    else:\n",
    "        A = theta / (2 * np.sin(theta)) * np.array([[R[2, 1] - R[1, 2]], [R[0, 2] - R[2, 0]], [R[1, 0] - R[0, 1]]])\n",
    "\n",
    "    return A\n",
    "\n",
    "def revert_coordinate_space(channels, R0, T0):\n",
    "    \"\"\"\n",
    "    Copied from https://github.com/una-dinosauria/human-motion-prediction\n",
    "    \"\"\"\n",
    "    n, d = channels.shape\n",
    "\n",
    "    channels_rec = copy.copy(channels)\n",
    "    R_prev = R0\n",
    "    T_prev = T0\n",
    "    rootRotInd = np.arange(3, 6)\n",
    "\n",
    "    # Loop through the passed posses\n",
    "    for ii in range(n):\n",
    "        R_diff = expmap2rotmat(channels[ii, rootRotInd])\n",
    "        R = R_diff.dot(R_prev)\n",
    "\n",
    "        channels_rec[ii, rootRotInd] = np.reshape(rotmat2expmap(R), 3)\n",
    "        T = T_prev + ((R_prev.T).dot(np.reshape(channels[ii, :3], [3, 1]))).reshape(-1)\n",
    "        channels_rec[ii, :3] = T\n",
    "        T_prev = T\n",
    "        R_prev = R\n",
    "\n",
    "    return channels_rec\n",
    "\n",
    "def rotmat2euler(R):\n",
    "    if R[0, 2] == 1 or R[0, 2] == -1:\n",
    "        E3 = 0\n",
    "        dlta = np.arctan2(R[0, 1], R[0, 2])\n",
    "        if R[0, 2] == -1:\n",
    "            E2 = np.pi/2\n",
    "            E1 = E3 + dlta\n",
    "        else:\n",
    "            E2 = -np.pi/2\n",
    "            E1 = -E3 + dlta\n",
    "    else:\n",
    "        E2 = -np.arcsin(R[0, 2])\n",
    "        E1 = np.arctan2(R[1, 2]/np.cos(E2), R[2, 2]/np.cos(E2))\n",
    "        E3 = np.arctan2(R[0, 1]/np.cos(E2), R[0, 0]/np.cos(E2))\n",
    "\n",
    "    eul = np.array([E1, E2, E3])\n",
    "\n",
    "    return eul\n",
    "\n",
    "def expmap2rotmat(A):\n",
    "    theta = np.linalg.norm(A)\n",
    "    if theta == 0:\n",
    "        R = np.identity(3)\n",
    "    else:\n",
    "        A = A / theta\n",
    "        cross_matrix = np.array([[0, -A[2], A[1]], [A[2], 0, -A[0]], [-A[1], A[0], 0]])\n",
    "        R = np.identity(3) + np.sin(theta) * cross_matrix + (1 - np.cos(theta)) * np.matmul(cross_matrix, cross_matrix)\n",
    "\n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0865d70-2931-4744-92e2-f35b1066a07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(config, checkpoint_dir, output_dir):\n",
    "\n",
    "    print('Start testing the Model!')\n",
    "\n",
    "    if not (os.path.exists(output_dir)):\n",
    "        os.makedirs(output_dir)\n",
    "    print(\"Outputs saved to: \" + output_dir)\n",
    "\n",
    "    config.output_window_size = 100\n",
    "        \n",
    "    choose = DatasetChooser(config)\n",
    "    if config.dataset == 'Human':\n",
    "        # This step is to get mean value, etc for unnorm\n",
    "        _, _ = choose(train=True)\n",
    "\n",
    "    if config.longterm is False:\n",
    "        prediction_dataset, bone_length = choose(prediction=True)\n",
    "        x_test, y_test, dec_in_test = prediction_dataset\n",
    "        actions = list(x_test.keys())\n",
    "    else:\n",
    "        # get raw validation data because the test data isn't usable\n",
    "        train_dataset, bone_length = choose(train=False)\n",
    "        test_set = train_dataset.data\n",
    "        x_test = {}\n",
    "        y_test = {}\n",
    "        dec_in_test = {}\n",
    "        test_set = test_set[0]\n",
    "        x_test[config.filename] = np.reshape(test_set[:config.input_window_size-1,:], [1, -1, config.input_size])\n",
    "        y_test[config.filename] = np.reshape(test_set[config.input_window_size:, :], [1, -1, config.input_size])\n",
    "        dec_in_test[config.filename] = np.reshape(test_set[config.input_window_size-1:-1, :], [1, -1, config.input_size])\n",
    "        config.output_window_size = y_test[config.filename].shape[1]\n",
    "        actions = [config.filename]\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else 'cuda:0')\n",
    "    print('Device {} will be used to save parameters'.format(device))\n",
    "    \n",
    "    net = choose_net(config)\n",
    "    net.to(device)\n",
    "    \n",
    "    dir = get_file_list(checkpoint_dir)\n",
    "    net.load_state_dict(torch.load(checkpoint_dir + dir[-1], map_location='cuda:0'))\n",
    "    \n",
    "    y_predict = {}\n",
    "    x_testk = {}\n",
    "    x_testl = {}\n",
    "    with torch.no_grad():\n",
    "        for act in actions:\n",
    "            #print(\"actions\", actions) #['directions', 'discussion', 'eating', 'greeting', 'phoning', 'posing', 'purchases', 'sitting', 'sittingdown', 'smoking', 'takingphoto', 'waiting', 'walking', 'walkingdog', 'walkingtogether']\n",
    "            x_test_ = torch.from_numpy(x_test[act]).float().to(device)\n",
    "            ref_pred_xtest = copy.deepcopy(prediction_dataset[0][act][0])\n",
    "            x_test_batch = torch.from_numpy(x_test[act]).float().to(device)\n",
    "            x_testl[act] = [lie_group_and_algebra_transform(frame,ref_pred_xtest) for frame in x_test_batch]\n",
    "            x_test_l = torch.tensor(x_testl[act], dtype=torch.float32, device=device)\n",
    "            x_test_batch = torch.from_numpy(x_test[act]).float().to(device)\n",
    "            x_testk[act] = [CenteredScaledd(frame) for frame in x_test_batch]\n",
    "            x_testk[act] = [inv_exp(ref_pred_xtest, frame) for frame in x_testk[act]]\n",
    "            x_test_k = torch.from_numpy(np.array(x_testk[act])).float().to(device)                         \n",
    "            dec_in_test_ = torch.from_numpy(dec_in_test[act]).float().to(device)    \n",
    "\n",
    "            pred = net(x_test_k, x_test_l, x_test_, dec_in_test_, batch_size, train_mode=False)\n",
    "                \n",
    "            # Assuming pred is a tensor or array\n",
    "            if isinstance(pred, torch.Tensor) or isinstance(pred, np.ndarray):\n",
    "                pred = pred.cpu().numpy() # Convert tensor to NumPy array\n",
    "            else:\n",
    "                print(\"pred is not a valid tensor or array.\")\n",
    "            y_predict[act] = pred\n",
    "                        \n",
    "    for act in actions:\n",
    "        if config.datatype == 'xyz':  \n",
    "            mean_error, _ = mean_per_joint_position_error(config, act, y_predict[act], y_test[act])\n",
    "            sio.savemat(output_dir + 'error_' + act + '.mat', dict([('error', mean_error)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0e7903-d9fb-4123-b7ff-35ee6ab06412",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.2, 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82166952-ea22-4ca8-96b4-59844ea07e92",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef802c3-36ee-412c-9d81-4b6f340b1d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Creating the model ....')\n",
    "model                       = Model(in_size, embed_size, hidden_size, dropout_val=dropout_val, batch_size=batch_size)\n",
    "model                       = nn.DataParallel(model).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0c6d56-ca26-41e9-a905-4acd1d6f7e98",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Train & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a8753d-2018-42b4-baed-07f555604048",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_directory(config):\n",
    "    \"\"\"\n",
    "    crate Checkpoint directory path\n",
    "    modified from https://github.com/BII-wushuang/Lie-Group-Motion-Prediction\n",
    "    \"\"\"\n",
    "    folder_dir = config.dataset + '/' + config.datatype + '_' + config.loss + 'loss_' + config.model\n",
    "    \n",
    "    if config.model == 'HMR':\n",
    "        folder_dir +='_RecurrentSteps='+str(config.encoder_recurrent_steps)+'_'+'ContextWindow='+str(config.context_window)+'_'+'hiddenSize='+ str(config.hidden_size)\n",
    "    if config.model == 'ST_HRN':\n",
    "        folder_dir +='_RecurrentSteps='+str(config.encoder_recurrent_steps)+'_hiddenSize='+str(config.hidden_size)+'_decoder_name='+ str(config.decoder)\n",
    "    if config.model == 'cmatp':\n",
    "        folder_dir +='_tf_kendall+Lie_Euc='+'_'+'ContextWindow='+str(config.context_window)+'_'+'hiddenSize='+ str(hidden_size)\n",
    "\n",
    "    folder_dir += '/' + config.filename + '/'\n",
    "    folder_dir += 'inputWindow=' + str(config.input_window_size) + '_outputWindow=' + str(config.output_window_size) + '/'\n",
    "\n",
    "    checkpoint_dir = './checkpoint/' + folder_dir\n",
    "    output_dir = './output/' + folder_dir\n",
    "\n",
    "    return [checkpoint_dir, output_dir]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c695df1b-ba41-48be-9571-4cd93fd292ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    gpu=[0]\n",
    "    training=False #test (True if train)\n",
    "    action='all'\n",
    "    dataset='Human'\n",
    "    datatype='xyz'\n",
    "    visualize=0\n",
    "\n",
    "    \n",
    "config = TrainConfig(args.dataset, args.datatype, args.action, args.gpu, args.training, args.visualize)\n",
    "checkpoint_dir, output_dir = create_directory(config)\n",
    "\n",
    "if config.train_model is True:\n",
    "    train(config, checkpoint_dir)\n",
    "else:\n",
    "    prediction(config, checkpoint_dir, output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
