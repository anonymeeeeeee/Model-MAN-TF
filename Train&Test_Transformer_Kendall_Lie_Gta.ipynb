{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b840bc4-7ae9-43be-a749-3a82825b5c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import dataloader,dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision.utils import save_image\n",
    "from torch.distributions.normal import Normal\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.transforms import ToTensor \n",
    "from torchsummary import summary\n",
    "import math\n",
    "import os\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import cv2\n",
    "from skimage.util import img_as_ubyte\n",
    "from skimage import io\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "import time\n",
    "from IPython import display\n",
    "import networkx as nx\n",
    "import glob\n",
    "import hashlib\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import scipy.io as sio\n",
    "import copy\n",
    "import sys\n",
    "import copy\n",
    "from utils import Progbar\n",
    "from loss import Loss\n",
    "import utils\n",
    "from argparse import ArgumentParser\n",
    "from scipy.linalg import expm\n",
    "from scipy.spatial import procrustes\n",
    "from models.indiv_crossAttention import crossAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5e5e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "torch.cuda.device_count()\n",
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a23721d-3fb1-45b9-a6c8-b2207d3ba594",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98389fe6-862b-4b05-82f5-4110ebb24c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "T_obs                   = 30\n",
    "T_pred                  = 60\n",
    "T_total                 = T_obs + T_pred\n",
    "batch_size              = 16\n",
    "in_size                 = 63\n",
    "out_size                = 21\n",
    "stochastic_out_size     = out_size * 3\n",
    "hidden_size             = 256\n",
    "embed_size              = 64\n",
    "global dropout_val\n",
    "dropout_val             = 0.2\n",
    "teacher_forcing_ratio   = 0.7\n",
    "avg_n_path_eval         = 20\n",
    "bst_n_path_eval         = 20\n",
    "path_mode               = \"avg\" \n",
    "startpoint_mode         = \"on\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b4e62a8-61de-4856-8f3b-5c3cb7d9072c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    #Model specific parameters\n",
    "    learn_A=True #Self learning Adjacecny matrix #learn_A=False\n",
    "    video_back = False #Use Sequence of image embedding \n",
    "    background_back = False #Use single image embedding\n",
    "    #Data specifc paremeters\n",
    "    obs_seq_len=30\n",
    "    pred_seq_len=60\n",
    "    dataset='GTA_IM'\n",
    "    im_w= 90\n",
    "    im_h= 160\n",
    "    \n",
    "    #Training specifc parameters\n",
    "    num_epochs=500\n",
    "    log_frq=32\n",
    "    batch_size=16\n",
    "    clip_grad=1.0   \n",
    "    lr=0.001\n",
    "    lr_sh_rate=200 \n",
    "    use_lrschd=True\n",
    "    tag=''\n",
    "    eval_only=False #evaluate the model\n",
    "    torso_joint=13 #center of torso (13 for GTA-IM)\n",
    "args=Args()\n",
    "\n",
    "#create a unique tag per exp\n",
    "original_tag = args.tag\n",
    "args_hash = ''\n",
    "for k,v in vars(args).items():\n",
    "    if k == 'eval_only' or k =='torso_joint':\n",
    "        continue\n",
    "    args_hash += str(k)+str(v)\n",
    "args_hash = hashlib.sha256(args_hash.encode()).hexdigest()\n",
    "args.tag+=args_hash\n",
    "\n",
    "input_window_size = 30 \n",
    "output_window_size = 60"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4e1d1d-b5af-4980-98ab-3fe7056367ee",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Dataloader_Gta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b250e17-47f3-40f6-b77e-e68b9b145f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class D2_D3_GTA_IM(Dataset):\n",
    "    def __init__(self, dpaths, tag='', seq_in=5, seq_out=10, load_img = True, load_depth =False, img_resize=(90,160), save= True):\n",
    "        super(D2_D3_GTA_IM, self).__init__()\n",
    "        self.load_img = load_img\n",
    "        self.load_depth = load_depth\n",
    "        self.seq_in = seq_in\n",
    "        self.seq_out= seq_out\n",
    "        self.save = save\n",
    "        self.img_resize = img_resize\n",
    "        self.tag = tag\n",
    "        \n",
    "        _hash = '-'.join(dpaths)+str(tag)+str(seq_in)+str(seq_out)+str(load_img)+str(load_depth)+str(img_resize[0])+str(img_resize[1]) #dpaths: the path of data\n",
    "        savefile = './'+hashlib.sha256(_hash.encode()).hexdigest()+'.pkl'\n",
    "        print(\"Save file is:\", savefile)\n",
    "        \n",
    "        if save:\n",
    "            if os.path.exists(savefile):\n",
    "                print(\"Save file:\",savefile,\" exists... loading it\")\n",
    "                with open(savefile,'rb') as f: \n",
    "                    data = pickle.load(f)\n",
    "                self._x = data['_x'] #x is the 2d input joins\n",
    "                self._xA = data['_xA'] #xA is the adjacency matrix\n",
    "                self._y = data['_y'] #y is the 3d target poses\n",
    "                self._x3 = data['_x3'] #x3 is the 3d input joins\n",
    "                if load_img:\n",
    "                    self._vrgb = data['_vrgb']\n",
    "                if load_depth:\n",
    "                    self._vdepth = data['_vdepth']\n",
    "                    \n",
    "            else:\n",
    "                #This need to be computed once, the Adjacency matrix of the skeleton \n",
    "                LIMBS = [\n",
    "                    (0, 1),  # head_center -> neck\n",
    "                    (1, 2),  # neck -> right_clavicle\n",
    "                    (2, 3),  # right_clavicle -> right_shoulder\n",
    "                    (3, 4),  # right_shoulder -> right_elbow\n",
    "                    (4, 5),  # right_elbow -> right_wrist\n",
    "                    (1, 6),  # neck -> left_clavicle\n",
    "                    (6, 7),  # left_clavicle -> left_shoulder\n",
    "                    (7, 8),  # left_shoulder -> left_elbow\n",
    "                    (8, 9),  # left_elbow -> left_wrist\n",
    "                    (1, 10),  # neck -> spine0\n",
    "                    (10, 11),  # spine0 -> spine1\n",
    "                    (11, 12),  # spine1 -> spine2\n",
    "                    (12, 13),  # spine2 -> spine3\n",
    "                    (13, 14),  # spine3 -> spine4\n",
    "                    (14, 15),  # spine4 -> right_hip\n",
    "                    (15, 16),  # right_hip -> right_knee\n",
    "                    (16, 17),  # right_knee -> right_ankle\n",
    "                    (14, 18),  # spine4 -> left_hip\n",
    "                    (18, 19),  # left_hip -> left_knee\n",
    "                    (19, 20)  # left_knee -> left_ankle\n",
    "                ]\n",
    "\n",
    "                A = np.zeros((21,21))\n",
    "                for i,j in LIMBS: \n",
    "                    A[i,j] = 1\n",
    "                    A[j,i] = 1\n",
    "\n",
    "                G = nx.from_numpy_matrix(A)\n",
    "                Anorm = nx.normalized_laplacian_matrix(G).toarray()\n",
    "\n",
    "                _x = []\n",
    "                _xA = []\n",
    "                if load_img:\n",
    "                    _vrgb = []\n",
    "                if load_depth:\n",
    "                    _vdepth = []\n",
    "                _y = []\n",
    "                _x3 = []\n",
    "\n",
    "                for dpath in dpaths:\n",
    "\n",
    "                    info = pickle.load(open(dpath + 'info_frames.pickle', 'rb'))\n",
    "                    info_npz = np.load(dpath+'info_frames.npz')\n",
    "\n",
    "                    if load_img:\n",
    "                        _rgb = []\n",
    "                    if load_depth:\n",
    "                        _depth = []\n",
    "              \n",
    "                    _d2 = [] \n",
    "                    _d2A = []\n",
    "                    _d3 = []\n",
    "                    for fm_id in range(len(info)):\n",
    "\n",
    "                        if load_img:\n",
    "                            rgb = cv2.resize(cv2.cvtColor(cv2.imread(dpath+'{:05d}'.format(fm_id)+'.jpg'), cv2.COLOR_BGR2RGB),img_resize, interpolation = cv2.INTER_LANCZOS4)/255.0\n",
    "\n",
    "                        if load_depth:\n",
    "                            depth = cv2.resize(cv2.cvtColor(cv2.imread(dpath+'{:05d}'.format(fm_id)+'.png'), cv2.COLOR_BGR2RGB),img_resize, interpolation = cv2.INTER_LANCZOS4)/255.0\n",
    "                        \n",
    "                        d2 = info_npz['joints_2d'][fm_id] \n",
    "                        d3 = info_npz['joints_3d_cam'][fm_id]\n",
    "\n",
    "                        if load_img:\n",
    "                            _rgb.append(rgb[None,...].transpose(0,3,1,2))\n",
    "                        if load_depth:\n",
    "                            _depth.append(depth[None,...].transpose(0,3,1,2))\n",
    "                        _d2.append(d2[None,...])\n",
    "                        _d2A.append(A[None,...])\n",
    "                        _d3.append(d3[None,...])\n",
    "\n",
    "                    #Create the sequences using a moving window of (seq_in_seq_out)\n",
    "                    kk =0 \n",
    "                    for k in range(0,len(info)-(seq_in+seq_out),1):\n",
    "                        kk =k\n",
    "\n",
    "                    pbar = tqdm(total=kk) \n",
    "\n",
    "                    for i in range(0,len(info)-(seq_in+seq_out),1):\n",
    "                        _x.append(torch.from_numpy(np.concatenate(_d2[i:i+seq_in],axis=0)).type(torch.float32))\n",
    "                        if load_img:\n",
    "                            _vrgb.append(torch.from_numpy(np.concatenate(_rgb[i:i+seq_in],axis=0)).type(torch.float32))\n",
    "                        if load_depth:\n",
    "                            _vdepth.append(torch.from_numpy(np.concatenate(_depth[i:i+seq_in],axis=0)).type(torch.float32))\n",
    "\n",
    "                        _xA.append(torch.from_numpy(np.concatenate(_d2A[i:i+seq_in],axis=0)).type(torch.float32))\n",
    "                        _y.append(torch.from_numpy(np.concatenate(_d3[i+seq_in:i+seq_in+seq_out],axis=0)).type(torch.float32))\n",
    "                        _x3.append(torch.from_numpy(np.concatenate(_d3[i:i+seq_in],axis=0)).type(torch.float32))\n",
    "                        pbar.update(1)\n",
    "                    pbar.close()\n",
    "\n",
    "                self._x = _x\n",
    "                self._xA = _xA\n",
    "                self._y = _y\n",
    "                self._x3 = _x3\n",
    "                if load_img:\n",
    "                    self._vrgb = _vrgb\n",
    "                if load_depth:\n",
    "                    self._vdepth = _vdepth\n",
    "                    \n",
    "                if save:\n",
    "                    with open(savefile,'wb') as f : \n",
    "                        data = {}\n",
    "                        data['_x'] = _x\n",
    "                        data['_xA'] = _xA\n",
    "                        data['_y'] = _y\n",
    "                        data['_x3'] = _x\n",
    "                        if load_img:\n",
    "                            data['_vrgb'] = _vrgb\n",
    "                        if load_depth:\n",
    "                            data['_vdepth'] = _vdepth\n",
    "                        pickle.dump(data,f)        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._x)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        output = [self._x[index],self._xA[index],self._y[index],self._x3[index]]\n",
    "        if self.load_img:\n",
    "            output.append(self._vrgb[index])\n",
    "        if self.load_depth:\n",
    "            output.append(self._vdepth[index])\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819a3168-2f76-478d-ab61-3540a3b78778",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.dataset == 'GTA_IM':\n",
    "    datasections = glob.glob('./GTAIM/*/')\n",
    "    load_img = False\n",
    "    if args.video_back or args.background_back:\n",
    "        load_img = True\n",
    "    dataset_train = D2_D3_GTA_IM(datasections[:8],tag='train', seq_in=args.obs_seq_len, seq_out=args.pred_seq_len,load_img = load_img, load_depth =False, img_resize=(args.im_w,args.im_h))    print(\"dataset_test\")\n",
    "    dataset_test = D2_D3_GTA_IM(datasections[8:10],tag='test', seq_in=args.obs_seq_len, seq_out=args.pred_seq_len,load_img = load_img, load_depth =False, img_resize=(args.im_w,args.im_h))\n",
    "\n",
    "elif args.dataset == 'PROX':\n",
    "    record_sections = glob.glob('PROX/recordings/*/')\n",
    "    keyp_sections = glob.glob('PROX/keypoints/*/')\n",
    "    load_img = False\n",
    "    if args.video_back or args.background_back:\n",
    "        load_img = True\n",
    "    dataset_train = D2_D3_PROX(record_sections[:52],keyp_sections[:52],tag='train', seq_in=args.obs_seq_len, seq_out=args.pred_seq_len,load_img = load_img, load_depth =False, img_resize=(args.im_w,args.im_h))\n",
    "    dataset_test = D2_D3_PROX(record_sections[52:60],keyp_sections[52:60],tag='test', seq_in=args.obs_seq_len, seq_out=args.pred_seq_len,load_img = load_img, load_depth =False, img_resize=(args.im_w,args.im_h))\n",
    "    \n",
    "loader_train = torch.utils.data.DataLoader(dataset_train, batch_size=args.batch_size, shuffle =True, num_workers=0, drop_last=True)\n",
    "loader_val = torch.utils.data.DataLoader(dataset_test, batch_size=args.batch_size, shuffle =False, num_workers=0, drop_last=True)\n",
    "\n",
    "print(\"Normalization\")\n",
    "all_train_2d = []\n",
    "for cnt,batch in enumerate(loader_train): \n",
    "    if args.background_back or args.video_back:\n",
    "        X,XA,y,x3,scene = batch\n",
    "    else:\n",
    "        X,XA,y,x3 = batch\n",
    "\n",
    "    all_train_2d.extend(X.flatten().numpy())\n",
    "    \n",
    "all_train_2d = np.asarray(all_train_2d)\n",
    "_mean  = all_train_2d.mean()\n",
    "_std = all_train_2d.std()\n",
    "\n",
    "print(\"vision normalization\")\n",
    "if args.background_back:\n",
    "    v_mean = [0.485, 0.456, 0.406]\n",
    "    v_std = [0.229, 0.224, 0.225]\n",
    "elif args.video_back:\n",
    "    v_mean = [0.43216, 0.394666, 0.37645]\n",
    "    v_std = [0.22803, 0.22145, 0.216989] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c80a6b-f01e-4edc-9cfb-c2c114279256",
   "metadata": {},
   "source": [
    "### Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b61daa4-1d43-4b1f-ac39-6796560c214d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_skel = copy.deepcopy(dataset_train._x3[0][0])\n",
    "\n",
    "#KENDALL:\n",
    "\n",
    "def CenteredScaled(X):\n",
    "    # Centering: Subtract the mean of each joint across all frames\n",
    "    X_reshaped = X - torch.mean(X, dim=0)\n",
    "    # Calculate the \"centered\" Frobenius norm for each joint\n",
    "    normX = torch.norm(X_reshaped, dim=(1, 2), p='fro')\n",
    "    # Scale to equal (unit) norm for each joint\n",
    "    X_scaled = X_reshaped / normX[:, None, None]\n",
    "    return X_scaled\n",
    "\n",
    "def inv_exp(X, Y):\n",
    "    # Move tensors to CPU for procrustes analysis\n",
    "    X_cpu = X.cpu().numpy()\n",
    "    Y_cpu = Y.cpu().numpy()\n",
    "    \n",
    "    n_frames, joints, dim_k = X.shape\n",
    "    num_frames = X_cpu.shape[0]\n",
    "    Total_dim = X_cpu.shape[1]*3\n",
    "    \n",
    "    X_cpu = X_cpu.reshape(num_frames,Total_dim)\n",
    "    Y_cpu = X_cpu.reshape(num_frames,Total_dim)\n",
    "    \n",
    "    # Check if X and Y have more than one row\n",
    "    if X_cpu.shape[0] <= 1 or Y_cpu.shape[0] <= 1:\n",
    "        # Handle the case when matrices have one row\n",
    "        pass\n",
    "        return Y\n",
    "\n",
    "    # Apply Procrustes to align Y on X\n",
    "    _, Y_aligned, _ = procrustes(X_cpu, Y_cpu)\n",
    "\n",
    "    # Calculate the invExp matrix on the aligned matrices\n",
    "    skeleton = X_cpu.dot(Y_aligned.T)\n",
    "    tr = abs(skeleton.trace())\n",
    "    if tr > 1:\n",
    "        tr = 1\n",
    "    teta_invexp = math.acos(tr)\n",
    "    if math.sin(teta_invexp) < 0.0001:\n",
    "        teta_invexp = 0.1\n",
    "    invExp = (teta_invexp / math.sin(teta_invexp)) * (Y_aligned - (math.cos(teta_invexp)) * X_cpu)\n",
    "    np_inv = np.array(invExp)\n",
    "    \n",
    "    # Reshape back to the original shape\n",
    "    np_inv = np_inv.reshape((n_frames, joints, dim_k))\n",
    "    \n",
    "    return torch.from_numpy(np_inv).to(X.device)\n",
    "\n",
    "\n",
    "#LIE: \n",
    "\n",
    "def calculate_global_transformation(skeleton, ref_skeleton):\n",
    "        \n",
    "    #Ensure skeleton and ref_skeleton have compatible shapes for the dot product\n",
    "    skeleton = skeleton.reshape(-1, 63) if skeleton.shape[1] == 63 else skeleton\n",
    "    ref_skeleton = ref_skeleton.reshape(-1, 63) if ref_skeleton.shape[1] == 63 else ref_skeleton\n",
    "\n",
    "    # Calculate global rotation\n",
    "    rotation_matrix = np.dot(skeleton, np.transpose(ref_skeleton))\n",
    "    u, s, v = np.linalg.svd(rotation_matrix, full_matrices=False)\n",
    "    rotation_matrix = np.dot(v.T, u.T)\n",
    "    # Calculate global translation\n",
    "    translation_vector = np.mean(ref_skeleton, axis=1) - np.dot(rotation_matrix, np.mean(skeleton, axis=1))\n",
    "    return rotation_matrix, translation_vector\n",
    "\n",
    "def to_SE3(rotation_matrix, translation_vector):\n",
    "    se3_matrix = np.eye(4)\n",
    "    se3_matrix[:3, :3] = rotation_matrix[:3, :3]  # Take the top-left 3x3 block\n",
    "    se3_matrix[:3, 3] = translation_vector[:3]  # Take the first 3 elements\n",
    "    return se3_matrix\n",
    "\n",
    "def extract_point_in_SE3(se3_matrix):\n",
    "    return se3_matrix\n",
    "\n",
    "def derive_tangent_space(rotation_matrix, translation_vector):\n",
    "    # Ensure rotation_matrix is 3x3\n",
    "    rotation_matrix = rotation_matrix[:3, :3]\n",
    "    # Create a 3x3 identity matrix\n",
    "    identity_matrix = np.eye(3)\n",
    "    # Ensure translation_vector is a column vector\n",
    "    translation_vector = translation_vector[:3].reshape(-1, 1)\n",
    "    # Calculate the skew-symmetric matrix directly\n",
    "    skew_symmetric_matrix = rotation_matrix - identity_matrix\n",
    "    skew_symmetric_matrix_flat = skew_symmetric_matrix.flatten()\n",
    "    tangent_space = np.zeros((4, 4))\n",
    "    tangent_space[:3, :3] = rotation_matrix\n",
    "    tangent_space[:3, 3] = translation_vector.flatten()\n",
    "    tangent_space[3, :3] = skew_symmetric_matrix_flat[:3]  # Take the first 3 elements\n",
    "    return tangent_space\n",
    "\n",
    "def lie_group_and_algebra_transform_s(skeleton, ref_skeleton):\n",
    "    skeleton = skeleton.cpu().numpy() if isinstance(skeleton, torch.Tensor) else skeleton\n",
    "    ref_skeleton = ref_skeleton.cpu().numpy() if isinstance(ref_skeleton, torch.Tensor) else ref_skeleton\n",
    "    # Calculate global transformation (rotation and translation)\n",
    "    rotation_matrix, translation_vector = calculate_global_transformation(skeleton, ref_skeleton)\n",
    "    se3_matrix = to_SE3(rotation_matrix, translation_vector)\n",
    "    # Extract a representative point in Lie group (SE(3))\n",
    "    point_in_SE3 = extract_point_in_SE3(se3_matrix)\n",
    "    # Derive tangent space (Lie algebra) associated with SE(3)\n",
    "    tangent_space = derive_tangent_space(rotation_matrix, translation_vector)\n",
    "    return tangent_space #point_in_SE3, tangent_space\n",
    "\n",
    "def lie_group_and_algebra_transform(frames, ref_skeleton):    \n",
    "    result = np.array([lie_group_and_algebra_transform_s(skeleton, ref_skeleton) for skeleton in frames])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dffe457-105c-461c-a6eb-bc1bcbe397d1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "26e048b8-34b7-4fc5-b493-a98e7814a7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def position_embedding(input, d_model):\n",
    "    input = input.view(-1, 1)\n",
    "    dim = torch.arange(d_model // 2, dtype=torch.float32, device=input.device).view(1, -1)\n",
    "    sin = torch.sin(input / 10000 ** (2 * dim / d_model))\n",
    "    cos = torch.cos(input / 10000 ** (2 * dim / d_model))\n",
    "\n",
    "    out = torch.zeros((input.shape[0], d_model), device=input.device)\n",
    "    out[:, ::2] = sin\n",
    "    out[:, 1::2] = cos\n",
    "    return out\n",
    "\n",
    "def sinusoid_encoding_table(max_len, d_model):\n",
    "    pos = torch.arange(max_len, dtype=torch.float32)\n",
    "    out = position_embedding(pos, d_model)\n",
    "    return out\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, d_model, d_k, d_v, h):\n",
    "        \"\"\"\n",
    "        param:\n",
    "        d_model: Output dimensionality of the model\n",
    "        d_k: Dimensionality of queries and keys\n",
    "        d_v: Dimensionality of values\n",
    "        h: Number of heads\n",
    "        \"\"\"\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.fc_q = nn.Linear(d_model, h * d_k)\n",
    "        self.fc_k = nn.Linear(d_model, h * d_k)\n",
    "        self.fc_v = nn.Linear(d_model, h * d_v)\n",
    "        self.fc_o = nn.Linear(h * d_v, d_model)\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.h = h\n",
    "\n",
    "        self.init_weights(gain=1.0)\n",
    "\n",
    "    def init_weights(self, gain=1.0):\n",
    "        nn.init.xavier_normal_(self.fc_q.weight, gain=gain)\n",
    "        nn.init.xavier_normal_(self.fc_k.weight, gain=gain)\n",
    "        nn.init.xavier_normal_(self.fc_v.weight, gain=gain)\n",
    "        nn.init.xavier_normal_(self.fc_o.weight, gain=gain)\n",
    "        nn.init.constant_(self.fc_q.bias, 0)\n",
    "        nn.init.constant_(self.fc_k.bias, 0)\n",
    "        nn.init.constant_(self.fc_v.bias, 0)\n",
    "        nn.init.constant_(self.fc_o.bias, 0)\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "        \"\"\"\n",
    "        Computes\n",
    "        :param queries: Queries (b_s, nq, d_model)\n",
    "        :param keys: Keys (b_s, nk, d_model)\n",
    "        :param values: Values (b_s, nk, d_model)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        b_s, nq = queries.shape[:2]\n",
    "        nk = keys.shape[1]\n",
    "        q = self.fc_q(queries).view(b_s, nq, self.h, self.d_k).permute(0, 2, 1, 3)  # (b_s, h, nq, d_k)\n",
    "        k = self.fc_k(keys).view(b_s, nk, self.h, self.d_k).permute(0, 2, 3, 1)  # (b_s, h, d_k, nk)\n",
    "        v = self.fc_v(values).view(b_s, nk, self.h, self.d_v).permute(0, 2, 1, 3)  # (b_s, h, nk, d_v)\n",
    "\n",
    "        att = torch.matmul(q, k) / np.sqrt(self.d_k)  # (b_s, h, nq, nk)\n",
    "\n",
    "        att = torch.softmax(att, -1)\n",
    "\n",
    "        out = torch.matmul(att, v).permute(0, 2, 1, 3).contiguous().view(b_s, nq, self.h * self.d_v)  # (b_s, nq, h*d_v)\n",
    "        out = self.fc_o(out)  # (b_s, nq, d_model)\n",
    "        return out\n",
    "    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, d_k, d_v, h, dff=2048, dropout=.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.attention = ScaledDotProductAttention(d_model=d_model, d_k=d_k, d_v=d_v, h=h)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "        self.fc = nn.Sequential(*[nn.Linear(d_model, dff), nn.ReLU(inplace=True), nn.Dropout(p=dropout),nn.Linear(dff, d_model)])\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "        att = self.attention(queries, keys, values)\n",
    "        att = self.dropout(att)\n",
    "        att = self.fc(att)\n",
    "        att = self.dropout(att)\n",
    "        return self.layer_norm(queries + att)\n",
    "    \n",
    "class EncoderSelfAttention(nn.Module):\n",
    "    def __init__(self, device, d_model, d_k, d_v, n_head, dff=2048, dropout_transformer=.1, n_module=6):\n",
    "        super(EncoderSelfAttention, self).__init__()\n",
    "        self.encoder = nn.ModuleList([MultiHeadAttention(d_model, d_k, d_v, n_head, dff, dropout_transformer) for _ in range(n_module)])\n",
    "        self.device = device\n",
    "    \n",
    "    def forward(self, x): \n",
    "        in_encoder = x + sinusoid_encoding_table(x.shape[1], x.shape[2]).expand(x.shape).to(self.device)\n",
    "        for l in self.encoder:\n",
    "            in_encoder = l(in_encoder, in_encoder, in_encoder)\n",
    "            \n",
    "        return in_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e38ec1b3-dcd5-4c5e-b0c1-a0d415457110",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.query = nn.Linear(input_dim, hidden_dim)\n",
    "        self.key = nn.Linear(input_dim, hidden_dim)\n",
    "        self.value = nn.Linear(input_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "\n",
    "        attention_scores = F.softmax(torch.matmul(q, k.transpose(-2, -1)) / (x.size(-1) ** 0.5), dim=-1)\n",
    "        output = torch.matmul(attention_scores, v)\n",
    "\n",
    "        return output\n",
    "\n",
    "class CoordinatesTransformer_k(nn.Module):\n",
    "    def __init__(self, device, dropout1d,input_dim=63, hidden_size=256, output_dim=256):\n",
    "        super(CoordinatesTransformer_k, self).__init__()\n",
    "        self.device = device\n",
    "        self.dropout1d = dropout1d\n",
    "        self.self_attention = SelfAttention(input_dim, hidden_size)\n",
    "        # Feedforward Layers\n",
    "        self.linear1 = nn.Linear(hidden_size, hidden_size) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout1d)\n",
    "        self.linear2 = nn.Linear(hidden_size, output_dim)\n",
    "\n",
    "    def forward(self, input_tensor_k):\n",
    "        self_attended = self.self_attention(input_tensor_k)\n",
    "        output = self.linear1(self_attended)\n",
    "        output = self.relu(output)\n",
    "        output = self.dropout(output)\n",
    "        output = self.linear2(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c0f22201-0a37-4c5c-a364-9399413bcae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.query = nn.Linear(input_dim, hidden_dim)\n",
    "        self.key = nn.Linear(input_dim, hidden_dim)\n",
    "        self.value = nn.Linear(input_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "        attention_scores = F.softmax(torch.matmul(q, k.transpose(-2, -1)) / (x.size(-1) ** 0.5), dim=-1)\n",
    "        output = torch.matmul(attention_scores, v)\n",
    "        return output\n",
    "\n",
    "class CoordinatesTransformer_l(nn.Module):\n",
    "    def __init__(self, device, dropout1d, input_dim=16, hidden_size=256, output_dim=256):\n",
    "        super(CoordinatesTransformer_l, self).__init__()\n",
    "        self.device = device\n",
    "        self.dropout1d = dropout1d\n",
    "        # Self-Attention Layer\n",
    "        self.self_attention = SelfAttention(input_dim, hidden_size)\n",
    "        # Feedforward Layers\n",
    "        self.linear1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout1d)\n",
    "        self.linear2 = nn.Linear(hidden_size, output_dim)\n",
    "\n",
    "    def forward(self, input_tensor_l):\n",
    "        # Assuming input_tensor_k shape: [batch_size, sequence_length, input_dim]\n",
    "        self_attended = self.self_attention(input_tensor_l)\n",
    "        output = self.linear1(self_attended)\n",
    "        output = self.relu(output)\n",
    "        output = self.dropout(output)\n",
    "        output = self.linear2(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "84acf023-828b-433a-8cb8-85f0d0ee1345",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead)\n",
    "        self.norm1 = nn.LayerNorm(d_model) \n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.feed_forward = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x, e_output):\n",
    "        # Multihead self-attention\n",
    "        attn_output, _ = self.self_attn(x, x, x)\n",
    "        x = x + self.norm1(attn_output)\n",
    "        # Feed forward\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = x + self.norm2(ff_output)\n",
    "        return x\n",
    "\n",
    "    \n",
    "class DecoderTransformer(nn.Module):\n",
    "    def __init__(self, in_size, embed_size, hidden_size, d_model=512, dropout_val=dropout_val, batch_size=1, nhead=8, num_layers=6):\n",
    "        super(DecoderTransformer, self).__init__()\n",
    "        self.in_size                = in_size \n",
    "        self.stochastic_out_size    = stochastic_out_size\n",
    "        self.hidden_size            = hidden_size\n",
    "        self.batch_size             = batch_size\n",
    "        self.embed_size             = embed_size\n",
    "        self.seq_length             = T_pred\n",
    "        self.dropout_val            = dropout_val\n",
    "        self.d_model                = d_model\n",
    "        self.nhead                  = nhead\n",
    "        self.num_layers             = num_layers\n",
    "\n",
    "        self.embedder_rho = nn.Linear(63, 200)\n",
    "        self.fC_mu = nn.Sequential(nn.Linear(self.hidden_size + self.hidden_size + 2, int(self.hidden_size/2), bias=True),nn.ReLU(),nn.Dropout(p=dropout_val),nn.Linear(int(self.hidden_size/2), self.stochastic_out_size, bias=True))\n",
    "        self.dropout = nn.Dropout(dropout_val)        \n",
    "        self.embedding = nn.Linear(200, d_model)\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model, nhead) for _ in range(num_layers)])\n",
    "        self.output_layer = nn.Linear(d_model, 514)\n",
    "                \n",
    "    def forward(self, x, encoder_outputs):        \n",
    "        embedding = self.embedder_rho(x.view(x.shape[0], 1, -1))                    \n",
    "        embedding = F.relu(self.dropout(embedding))        \n",
    "        x = self.embedding(embedding)\n",
    "        for layer in self.layers: #6 layers\n",
    "            x = layer(x, encoder_outputs)\n",
    "            if torch.isnan(x).any():\n",
    "                print(\"NaN values found in x layer\")\n",
    "            if torch.isinf(x).any():\n",
    "                print(\"infinite values found in x layer\")\n",
    "        output = self.output_layer(x)\n",
    "        prediction = self.fC_mu(output.squeeze(0))         \n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ff4caccf-5e65-420f-8fea-85b8b3dcc6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, in_size, embed_size, hidden_size, batch_size, d_model=512, d_ff=2048, h=8, dropout_val=dropout_val, N=6, input_dim=512):\n",
    "        super(Model, self).__init__()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        self.encoder_k = CoordinatesTransformer_k(device, dropout1d=dropout_val)\n",
    "        self.encoder_k.apply(init_weights)\n",
    "        self.encoder_l = CoordinatesTransformer_l(device, dropout1d=dropout_val)\n",
    "        self.encoder_l.apply(init_weights)\n",
    "        self.decoder = DecoderTransformer(in_size, embed_size, hidden_size, num_layers=6, nhead=8)\n",
    "        self.decoder.apply(init_weights)\n",
    "        self.crossAttention = crossAttention(N=6,d_model=256, d_ff=2048, h=8, dropout=0.1)\n",
    "                    \n",
    "        if device.type=='cuda':\n",
    "            self.encoder_k.cuda()\n",
    "            self.encoder_l.cuda()\n",
    "            self.decoder.cuda()\n",
    "\n",
    "    def forward(self, input_tensor_k, input_tensor_l, input_tensor, output_tensor, batch_size, train_mode):       \n",
    "  \n",
    "        batch_size     = int(input_tensor_k.size(0))\n",
    "        encoder_outputs = torch.zeros(batch_size, T_obs, hidden_size).cuda()\n",
    "        start_point = (input_tensor[:,0,:]).to(device).clone().detach()\n",
    "        if startpoint_mode==\"on\":\n",
    "            input_tensor[:,0,:]    = 0\n",
    "        \n",
    "        encoder_outputs_k = self.encoder_k(input_tensor_k)\n",
    "        encoder_outputs_l = self.encoder_l(input_tensor_l)\n",
    "\n",
    "        src_mask = None\n",
    "        obd_enc_mask = None\n",
    "        cross_ouput = self.crossAttention( encoder_outputs_k, encoder_outputs_l, src_mask, obd_enc_mask)\n",
    "        e_outputs=cross_ouput\n",
    "        \n",
    "        decoder_input = input_tensor[:,-1,:] \n",
    "        outputs                         = torch.zeros(batch_size, T_pred , in_size).cuda() \n",
    "        stochastic_outputs              = torch.zeros(batch_size, T_pred , stochastic_out_size).cuda()\n",
    "        teacher_force                   = 1\n",
    "        epsilonX                        = Normal(torch.zeros(batch_size,1),torch.ones(batch_size,1))\n",
    "        teacher_force                   = int(random.random() < teacher_forcing_ratio) if train_mode else 0\n",
    "\n",
    "        for t in range(0, output_window_size-1):\n",
    "            output = self.decoder(decoder_input, e_outputs)                    \n",
    "            # Reparameterization Trick :)\n",
    "            decoder_output              = torch.zeros(batch_size,1,63).cuda()                        \n",
    "            for i in range(0,out_size):\n",
    "                epsilonX               = Normal(torch.zeros(batch_size,1),torch.ones(batch_size,1))\n",
    "                decoder_output[:,:,i]  = output[:,:,2*i] + epsilonX.sample((avg_n_path_eval,1)).view(-1,avg_n_path_eval,1).mean(-2).cuda() * output[:,:,2*i+1]\n",
    "            outputs[:,t,:]               = decoder_output.squeeze(1)\n",
    "            stochastic_outputs[:,t,:]    = output.squeeze(1)\n",
    "            predictionns = outputs\n",
    "            predictionns = outputs.reshape(args.batch_size,T_pred,21,3)\n",
    "            \n",
    "        return predictionns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40037130",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.2, 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aedc445-68a5-4f1f-980c-c422ca5bb136",
   "metadata": {
    "tags": []
   },
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "052dbdc8-6eba-4846-af10-ad4023f0e952",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    global metrics,constant_metrics\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    net = Model(in_size, embed_size, hidden_size, dropout_val=dropout_val, batch_size=batch_size)\n",
    "    net.to(device)\n",
    "    \n",
    "    model.train()\n",
    "    loss_train = 0 \n",
    "\n",
    "    for cnt,batch in enumerate(loader_train): \n",
    "        #get data\n",
    "        if args.background_back:\n",
    "            X,XA,y,x3,scene = batch\n",
    "            scene =scene[:,-1,...].cuda()\n",
    "        elif args.video_back:\n",
    "            X,XA,y,x3,scene = batch\n",
    "            scene = scene.view(scene.shape[0],scene.shape[1],scene.shape[2],scene.shape[3],scene.shape[4]).cuda()\n",
    "        else: \n",
    "            X,XA,y,x3 = batch\n",
    "\n",
    "        X =(X-_mean)/_std\n",
    "        X,XA,y,x3 = X.cuda(),XA.cuda(),y.cuda(),x3.cuda()\n",
    "        \n",
    "        encoder_inputs = x3.reshape(args.batch_size, args.obs_seq_len,-1)\n",
    "        # Convert to Kendall shape space representations:\n",
    "        ref_skel = copy.deepcopy(dataset_train._x3[0])\n",
    "        x3_k = [CenteredScaled(frame) for frame in x3]\n",
    "        x3_k = torch.stack(x3_k)\n",
    "        x3_k_tg = [inv_exp(ref_skel,frame) for frame in x3_k]\n",
    "        x3_k_tg = torch.stack(x3_k_tg)\n",
    "        #print(\"x3_k_tg shape:\", x3_k_tg.shape) #torch.Size([16, 30, 21, 3])\n",
    "        encoder_inputs_k = x3_k_tg.reshape(args.batch_size, args.obs_seq_len,-1)\n",
    "\n",
    "        # Convert to Lie group and then Lie algebra:\n",
    "        ref_skel_l = copy.deepcopy(dataset_train._x3[0][0])\n",
    "        x3_l_tg = [lie_group_and_algebra_transform(frame,ref_skel_l) for frame in x3]\n",
    "        x3_l_tg = np.array(x3_l_tg)\n",
    "        x3_l_tg= torch.tensor(x3_l_tg)\n",
    "        #print(\"x3_l_tg_tensor shape:\", x3_l_tg.shape) #torch.Size([16, 30, 4, 4])\n",
    "        encoder_inputs_l = x3_l_tg.reshape(args.batch_size, args.obs_seq_len, -1)        \n",
    "    \n",
    "        decoder_outputs = y.view(args.batch_size, args.pred_seq_len,63)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        encoder_inputs_k = encoder_inputs_k.float().to(device)\n",
    "        encoder_inputs_l = encoder_inputs_l.float().to(device)\n",
    "        encoder_inputs = encoder_inputs.float().to(device)\n",
    "        decoder_outputs = decoder_outputs.float().to(device)\n",
    "        \n",
    "        prediction= net(encoder_inputs_k, encoder_inputs_l, encoder_inputs, decoder_outputs, batch_size,train_mode=True)\n",
    "        \n",
    "        loss = loss(prediction,y)\n",
    "        \n",
    "        loss.backward()\n",
    "        if args.clip_grad is not None:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(),args.clip_grad)\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_train += loss.item()\n",
    "        if cnt%args.log_frq == 0 and cnt!=0:\n",
    "            print('Epoch:', epoch,'\\t Train Loss:',loss_train/(cnt+1))\n",
    "            \n",
    "    metrics['train_loss'].append(loss_train/(cnt+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d309fa64-f96f-4fcb-82cb-b01b81bf7140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "\n",
    "def MPJPE(V_pred,V_trgt):\n",
    "    return torch.linalg.norm(V_trgt- V_pred,dim=-1).mean() \n",
    "\n",
    "def MPJPE_torso(V_pred,V_trgt,torso_joint = 13): \n",
    "    return torch.linalg.norm(V_trgt[:,:,torso_joint,:]- V_pred[:,:,torso_joint,:],dim=-1).mean() \n",
    "\n",
    "def MPJPE_timelimit(V_pred,V_trgt,time_limit):\n",
    "    T = V_pred.shape[1]\n",
    "    T = max(int(T*time_limit),1)\n",
    "    return torch.linalg.norm(V_trgt[:,:T,...]- V_pred[:,:T,...],dim=-1).mean()\n",
    "\n",
    "def MPJPE_torso_timelimit(V_pred,V_trgt,time_limit,torso_joint = 13 ): \n",
    "    T = V_pred.shape[1]\n",
    "    T = max(int(T*time_limit),1)\n",
    "    return torch.linalg.norm(V_trgt[:,:T,torso_joint,:]- V_pred[:,:T,torso_joint,:],dim=-1).mean() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfa885e-8bd8-4a73-bfeb-c40b98eb02f5",
   "metadata": {},
   "source": [
    "Vald"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f98edb83-40da-4dd4-80a2-3a3aa5aea86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vald(epoch):\n",
    "    global metrics,constant_metrics\n",
    "    \n",
    "    model.eval()\n",
    "    loss_val = 0 \n",
    "    mpjpe_avg = 0 \n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    net = Model(in_size, embed_size, hidden_size, dropout_val=dropout_val, batch_size=batch_size)\n",
    "    net.to(device)\n",
    "\n",
    "    with torch.no_grad(): #Faster without grad \n",
    "        for cnt,batch in enumerate(loader_val): \n",
    "            if args.background_back:\n",
    "                X,XA,y,x3,scene = batch\n",
    "                scene =scene[:,-1,...].cuda()\n",
    "            elif args.video_back:\n",
    "                X,XA,y,x3,scene = batch\n",
    "                scene = scene.view(scene.shape[0],scene.shape[1],scene.shape[2],scene.shape[3],scene.shape[4]).cuda()\n",
    "            else: \n",
    "                X,XA,y,x3 = batch\n",
    "\n",
    "            X =(X-_mean)/_std\n",
    "            X,XA,y,x3 = X.cuda(),XA.cuda(),y.cuda(),x3.cuda()\n",
    "            \n",
    "            encoder_inputs = x3.reshape(args.batch_size, args.obs_seq_len,-1)\n",
    "            ref_skel = copy.deepcopy(dataset_train._x3[0])\n",
    "            x3_k = [CenteredScaled(frame) for frame in x3]\n",
    "            x3_k = torch.stack(x3_k)\n",
    "            x3_k_tg = [inv_exp(ref_skel,frame) for frame in x3_k]\n",
    "            x3_k_tg = torch.stack(x3_k_tg)\n",
    "            encoder_inputs_k = x3_k_tg.reshape(args.batch_size, args.obs_seq_len,-1)\n",
    "            ref_skel_l = copy.deepcopy(dataset_train._x3[0][0])\n",
    "            x3_l_tg = [lie_group_and_algebra_transform(frame,ref_skel_l) for frame in x3]\n",
    "            x3_l_tg = np.array(x3_l_tg)\n",
    "            x3_l_tg= torch.tensor(x3_l_tg)\n",
    "            encoder_inputs_l = x3_l_tg.reshape(args.batch_size, args.obs_seq_len, -1)\n",
    "            decoder_outputs = y.view(args.batch_size, args.pred_seq_len,63)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            encoder_inputs_k = encoder_inputs_k.float().to(device)\n",
    "            encoder_inputs_l = encoder_inputs_l.float().to(device)\n",
    "            encoder_inputs = encoder_inputs.float().to(device)\n",
    "            decoder_outputs = decoder_outputs.float().to(device)\n",
    "\n",
    "            prediction= net(encoder_inputs_k, encoder_inputs_l, encoder_inputs, decoder_outputs, batch_size,train_mode=0)\n",
    "        \n",
    "            loss = loss(prediction,y)\n",
    "            mpjpe = MPJPE(prediction,y)\n",
    "            \n",
    "            loss_val += loss.item()\n",
    "            mpjpe_avg += mpjpe.item()\n",
    "            \n",
    "            if cnt%args.log_frq == 0 and cnt!=0:\n",
    "                print('Epoch:', epoch,'\\t Val Loss:',loss_val/(cnt+1),'\\t MPJPE:',mpjpe_avg/(cnt+1))\n",
    "\n",
    "    metrics['val_loss'].append(loss_val/(cnt+1))\n",
    "    metrics['val_mpjpe'].append(mpjpe_avg/(cnt+1))\n",
    "\n",
    "    if  metrics['val_loss'][-1]< constant_metrics['min_val_loss']:\n",
    "        constant_metrics['min_val_loss'] =  metrics['val_loss'][-1]\n",
    "        constant_metrics['min_val_epoch'] = epoch\n",
    "        torch.save(model.state_dict(),checkpoint_dir+'val_loss_best.pth')  \n",
    "        \n",
    "    if  metrics['val_mpjpe'][-1]< constant_metrics['min_val_mpjpe']:\n",
    "        constant_metrics['min_val_mpjpe'] =  metrics['val_mpjpe'][-1]\n",
    "        constant_metrics['min_val_mpjpe_epoch'] = epoch\n",
    "        torch.save(model.state_dict(),checkpoint_dir+'val_mpjpe_best.pth')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82166952-ea22-4ca8-96b4-59844ea07e92",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aef802c3-36ee-412c-9d81-4b6f340b1d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the model ....\n"
     ]
    }
   ],
   "source": [
    "print('Creating the model ....')\n",
    "model                       = Model(in_size, embed_size, hidden_size, dropout_val=dropout_val, batch_size=batch_size)\n",
    "model                       = nn.DataParallel(model).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0c6d56-ca26-41e9-a905-4acd1d6f7e98",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Train / Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc2c856-51ae-4ed5-8dda-d5d6b324cca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=args.lr) #optimizer = optim.SGD(model.parameters(),lr=args.lr)\n",
    "\n",
    "if args.use_lrschd: #Use lr rate scheduler\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=args.lr_sh_rate, gamma=0.2)\n",
    "    \n",
    "checkpoint_dir = './checkpoint/'+args.tag+'/'\n",
    "\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "    \n",
    "with open(checkpoint_dir+'args.pkl', 'wb') as fp:\n",
    "    pickle.dump(args, fp)\n",
    "    \n",
    "print('Data and model loaded')\n",
    "print('Checkpoint dir:', checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af848334",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.video_back = False \n",
    "args.background_back = False\n",
    "args.torso_joint=13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3370548-d223-4dc7-b674-b2f870f0c056",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train or eval? \n",
    "if not args.eval_only:\n",
    "    print('Training started ...')\n",
    "\n",
    "    metrics = {'train_loss':[],  'val_loss':[], 'val_mpjpe':[]}\n",
    "    constant_metrics = {'min_val_epoch':-1, 'min_val_loss':9999999999999999,'min_val_mpjpe_epoch':-1, 'min_val_mpjpe':9999999999999999}\n",
    "\n",
    "    for epoch in range(args.num_epochs):\n",
    "    \n",
    "        with torch.autograd.set_detect_anomaly(True):\n",
    "            train(epoch)\n",
    "        vald(epoch)\n",
    "                    \n",
    "        if args.use_lrschd:\n",
    "            scheduler.step()\n",
    "\n",
    "        print('*'*30)\n",
    "        print('Epoch:',args.tag,\":\", epoch)\n",
    "        for k,v in metrics.items():\n",
    "            if len(v)>0:\n",
    "                print(k,v[-1])\n",
    "\n",
    "        print(constant_metrics)\n",
    "        print('*'*30)\n",
    "\n",
    "        with open(checkpoint_dir+'metrics.pkl', 'wb') as fp:\n",
    "            pickle.dump(metrics, fp)\n",
    "\n",
    "        with open(checkpoint_dir+'constant_metrics.pkl', 'wb') as fp:\n",
    "            pickle.dump(constant_metrics, fp) \n",
    "else:\n",
    "    def eval_metrics():\n",
    "        model.eval()\n",
    "        loss_val = 0\n",
    "        \n",
    "        mpjpe_avg = 0 \n",
    "        mpjpe_qaurt_avg =0 \n",
    "        mpjpe_half_avg  =0\n",
    "        mpjpe_3quart_avg =0 \n",
    "\n",
    "        mpjpe_path_avg = 0 \n",
    "        mpjpe_path_qaurt_avg =0 \n",
    "        mpjpe_path_half_avg  =0\n",
    "        mpjpe_path_3quart_avg =0 \n",
    "\n",
    "        with torch.no_grad(): #Faster without grad \n",
    "            for cnt,batch in enumerate(loader_val): \n",
    "                if args.background_back:\n",
    "                    X,XA,y,x3,scene = batch\n",
    "                    scene =scene[:,0,...].cuda()\n",
    "                elif args.video_back:\n",
    "                    X,XA,y,x3,scene = batch\n",
    "                    scene = scene.view(scene.shape[0],scene.shape[1]*scene.shape[2],scene.shape[3],scene.shape[4]).cuda()\n",
    "                else: \n",
    "                    X,XA,y,x3 = batch\n",
    "\n",
    "                X =(X-_mean)/_std\n",
    "                X,XA,y,x3 = X.cuda(),XA.cuda(),y.cuda(),x3.cuda()\n",
    "                \n",
    "                encoder_inputs = x3.reshape(args.batch_size, args.obs_seq_len,-1)\n",
    "\n",
    "                ref_skel = copy.deepcopy(dataset_train._x3[0])\n",
    "                x3_k = [CenteredScaled(frame) for frame in x3]\n",
    "                x3_k = torch.stack(x3_k)\n",
    "                x3_k_tg = [inv_exp(ref_skel,frame) for frame in x3_k]\n",
    "                x3_k_tg = torch.stack(x3_k_tg)\n",
    "                encoder_inputs_k = x3_k_tg.reshape(args.batch_size, args.obs_seq_len,-1) #torch.Size([16, 50, 63])okk\n",
    "\n",
    "                ref_skel_l = copy.deepcopy(dataset_train._x3[0][0])\n",
    "                x3_l_tg = [lie_group_and_algebra_transform(frame,ref_skel_l) for frame in x3]\n",
    "                x3_l_tg = np.array(x3_l_tg)\n",
    "                x3_l_tg= torch.tensor(x3_l_tg)\n",
    "                encoder_inputs_l = x3_l_tg.reshape(args.batch_size, args.obs_seq_len, -1)\n",
    "\n",
    "                decoder_outputs = y.view(args.batch_size, args.pred_seq_len,63)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                encoder_inputs_k = encoder_inputs_k.float().to(device)\n",
    "                encoder_inputs_l = encoder_inputs_l.float().to(device)\n",
    "                encoder_inputs = encoder_inputs.float().to(device)\n",
    "                decoder_outputs = decoder_outputs.float().to(device)\n",
    "\n",
    "                prediction= net(encoder_inputs_k, encoder_inputs_l, encoder_inputs, decoder_outputs, batch_size,train_mode=0)\n",
    "            \n",
    "                loss_val += loss(prediction,y).item()\n",
    "                \n",
    "                mpjpe_avg+= MPJPE(prediction,y).item()\n",
    "                mpjpe_qaurt_avg+=MPJPE_timelimit(prediction,y,0.25).item()\n",
    "                mpjpe_half_avg+=MPJPE_timelimit(prediction,y,0.50).item()\n",
    "                mpjpe_3quart_avg+=MPJPE_timelimit(prediction,y,0.75).item()\n",
    "                \n",
    "                mpjpe_path_avg+= MPJPE_torso(prediction,y,torso_joint=torso_joint).item() \n",
    "                mpjpe_path_qaurt_avg+=MPJPE_torso_timelimit(prediction,y,0.25,torso_joint=torso_joint).item()  \n",
    "                mpjpe_path_half_avg+=MPJPE_torso_timelimit(prediction,y,0.50,torso_joint=torso_joint).item() \n",
    "                mpjpe_path_3quart_avg+=MPJPE_torso_timelimit(prediction,y,0.75,torso_joint=torso_joint).item()  \n",
    "                \n",
    "        loss_val /= (cnt+1)\n",
    "        mpjpe_avg /= (cnt+1)\n",
    "        mpjpe_qaurt_avg /= (cnt+1)\n",
    "        mpjpe_half_avg /= (cnt+1)\n",
    "        mpjpe_3quart_avg /= (cnt+1)\n",
    "\n",
    "        mpjpe_path_avg /= (cnt+1)\n",
    "        mpjpe_path_qaurt_avg /= (cnt+1)\n",
    "        mpjpe_path_half_avg  /= (cnt+1)\n",
    "        mpjpe_path_3quart_avg /= (cnt+1)\n",
    "        \n",
    "        mpjpe_avg = int(mpjpe_avg*1000)\n",
    "        mpjpe_qaurt_avg = int(mpjpe_qaurt_avg*1000)\n",
    "        mpjpe_half_avg = int(mpjpe_half_avg*1000)\n",
    "        mpjpe_3quart_avg = int(mpjpe_3quart_avg*1000)\n",
    "\n",
    "        mpjpe_path_avg = int(mpjpe_path_avg*1000)\n",
    "        mpjpe_path_qaurt_avg = int(mpjpe_path_qaurt_avg*1000)\n",
    "        mpjpe_path_half_avg  =int(mpjpe_path_half_avg*1000)\n",
    "        mpjpe_path_3quart_avg = int(mpjpe_path_3quart_avg*1000)\n",
    "                \n",
    "        print('#'*30)\n",
    "        print('All results are in mm')\n",
    "        print('*'*30)\n",
    "        print('MPJPE POSE: 0.25\\t 0.50\\t 0.75\\t full')\n",
    "        print('MPJPE: ',mpjpe_qaurt_avg,'\\t ',mpjpe_half_avg,'\\t ',mpjpe_3quart_avg,'\\t ',mpjpe_avg,'')\n",
    "        print('*'*30)\n",
    "        print('MPJPE PATH: 0.25\\t 0.50\\t 0.75\\t full')\n",
    "        print('PATH: ',mpjpe_path_qaurt_avg,'\\t ',mpjpe_path_half_avg,'\\t ',mpjpe_path_3quart_avg,'\\t ',mpjpe_path_avg,'')\n",
    "        print('#'*30)\n",
    "        print('#'*30)\n",
    "\n",
    "        f = open(checkpoint_dir+\"eval.csv\", \"w\")\n",
    "        eval_id = ''\n",
    "        for k,v in vars(args).items():\n",
    "            if k == 'eval_only' or k =='torso_joint' or k =='tag':\n",
    "                continue\n",
    "            eval_id += str(k)+str(v)\n",
    "        eval_result = [eval_id,',',original_tag,',',mpjpe_path_qaurt_avg,',',mpjpe_path_half_avg,',',mpjpe_path_3quart_avg,',',mpjpe_path_avg,',',mpjpe_qaurt_avg,',',mpjpe_half_avg,',',mpjpe_3quart_avg,',',mpjpe_avg,',',FDE,',',ADE,',',STB,'\\n']\n",
    "        eval_result_row = ''\n",
    "        for ss in eval_result:\n",
    "            eval_result_row+= str(ss)\n",
    "        f.write(eval_result_row)\n",
    "        f.close()\n",
    "    model.load_state_dict(torch.load(checkpoint_dir+'val_mpjpe_best.pth'))\n",
    "    eval_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f5720f-7df7-424c-b4e5-3df98fe243c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_metrics():\n",
    "    model.eval()\n",
    "    loss_val = 0\n",
    "\n",
    "    mpjpe_avg = 0 \n",
    "    mpjpe_qaurt_avg =0 \n",
    "    mpjpe_half_avg  =0\n",
    "    mpjpe_3quart_avg =0 \n",
    "\n",
    "    mpjpe_path_avg = 0 \n",
    "    mpjpe_path_qaurt_avg =0 \n",
    "    mpjpe_path_half_avg  =0\n",
    "    mpjpe_path_3quart_avg =0 \n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    net = Model(in_size, embed_size, hidden_size, dropout_val=dropout_val, batch_size=batch_size)\n",
    "    net.to(device)\n",
    "\n",
    "    with torch.no_grad(): #Faster without grad \n",
    "        for cnt,batch in enumerate(loader_val): \n",
    "            if args.background_back:\n",
    "                X,XA,y,x3,scene = batch\n",
    "                scene =scene[:,0,...].cuda()\n",
    "            elif args.video_back:\n",
    "                X,XA,y,x3,scene = batch\n",
    "                scene = scene.view(scene.shape[0],scene.shape[1],scene.shape[2],scene.shape[3],scene.shape[4]).cuda()\n",
    "            else: \n",
    "                X,XA,y,x3 = batch\n",
    "\n",
    "            X =(X-_mean)/_std\n",
    "            X,XA,y,x3 = X.cuda(),XA.cuda(),y.cuda(),x3.cuda()\n",
    "            \n",
    "            encoder_inputs = x3.reshape(args.batch_size, args.obs_seq_len,-1)\n",
    "            ref_skel = copy.deepcopy(dataset_train._x3[0])\n",
    "            x3_k = [CenteredScaled(frame) for frame in x3]\n",
    "            x3_k = torch.stack(x3_k)\n",
    "            x3_k_tg = [inv_exp(ref_skel,frame) for frame in x3_k]\n",
    "            x3_k_tg = torch.stack(x3_k_tg)\n",
    "            encoder_inputs_k = x3_k_tg.reshape(args.batch_size, args.obs_seq_len,-1) #torch.Size([16, 50, 63])okk\n",
    "            ref_skel_l = copy.deepcopy(dataset_train._x3[0][0])\n",
    "            x3_l_tg = [lie_group_and_algebra_transform(frame,ref_skel_l) for frame in x3]\n",
    "            x3_l_tg = np.array(x3_l_tg)\n",
    "            x3_l_tg= torch.tensor(x3_l_tg)\n",
    "            encoder_inputs_l = x3_l_tg.reshape(args.batch_size, args.obs_seq_len, -1)\n",
    "            decoder_outputs = y.view(args.batch_size, args.pred_seq_len,63)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            encoder_inputs_k = encoder_inputs_k.float().to(device)\n",
    "            encoder_inputs_l = encoder_inputs_l.float().to(device)\n",
    "            encoder_inputs = encoder_inputs.float().to(device)\n",
    "            decoder_outputs = decoder_outputs.float().to(device)\n",
    "\n",
    "            prediction= net(encoder_inputs_k, encoder_inputs_l, encoder_inputs, decoder_outputs, batch_size,train_mode=0)\n",
    "                                    \n",
    "            loss_val += loss(prediction,y).item()\n",
    "\n",
    "            mpjpe_avg+= MPJPE(prediction,y).item()\n",
    "            mpjpe_qaurt_avg+=MPJPE_timelimit(prediction,y,0.25).item()\n",
    "            mpjpe_half_avg+=MPJPE_timelimit(prediction,y,0.50).item()\n",
    "            mpjpe_3quart_avg+=MPJPE_timelimit(prediction,y,0.75).item()\n",
    "\n",
    "            mpjpe_path_avg+= MPJPE_torso(prediction,y,torso_joint=args.torso_joint).item() \n",
    "            mpjpe_path_qaurt_avg+=MPJPE_torso_timelimit(prediction,y,0.25,torso_joint=args.torso_joint).item()  \n",
    "            mpjpe_path_half_avg+=MPJPE_torso_timelimit(prediction,y,0.50,torso_joint=args.torso_joint).item() \n",
    "            mpjpe_path_3quart_avg+=MPJPE_torso_timelimit(prediction,y,0.75,torso_joint=args.torso_joint).item()  \n",
    "\n",
    "    loss_val /= (cnt+1)\n",
    "    mpjpe_avg /= (cnt+1)\n",
    "    mpjpe_qaurt_avg /= (cnt+1)\n",
    "    mpjpe_half_avg /= (cnt+1)\n",
    "    mpjpe_3quart_avg /= (cnt+1)\n",
    "\n",
    "    mpjpe_path_avg /= (cnt+1)\n",
    "    mpjpe_path_qaurt_avg /= (cnt+1)\n",
    "    mpjpe_path_half_avg  /= (cnt+1)\n",
    "    mpjpe_path_3quart_avg /= (cnt+1)\n",
    "\n",
    "    mpjpe_avg = int(mpjpe_avg*1000)\n",
    "    mpjpe_qaurt_avg = int(mpjpe_qaurt_avg*1000)\n",
    "    mpjpe_half_avg = int(mpjpe_half_avg*1000)\n",
    "    mpjpe_3quart_avg = int(mpjpe_3quart_avg*1000)\n",
    "\n",
    "    mpjpe_path_avg = int(mpjpe_path_avg*1000)\n",
    "    mpjpe_path_qaurt_avg = int(mpjpe_path_qaurt_avg*1000)\n",
    "    mpjpe_path_half_avg  =int(mpjpe_path_half_avg*1000)\n",
    "    mpjpe_path_3quart_avg = int(mpjpe_path_3quart_avg*1000)\n",
    "\n",
    "    print('#'*30)\n",
    "    print('All results are in mm')\n",
    "    print('*'*30)\n",
    "    print('MPJPE POSE: 0.25\\t 0.50\\t 0.75\\t full')\n",
    "    print('MPJPE: ',mpjpe_qaurt_avg,'\\t ',mpjpe_half_avg,'\\t ',mpjpe_3quart_avg,'\\t ',mpjpe_avg,'')\n",
    "    print('*'*30)\n",
    "    print('MPJPE PATH: 0.25\\t 0.50\\t 0.75\\t full')\n",
    "    print('PATH: ',mpjpe_path_qaurt_avg,'\\t ',mpjpe_path_half_avg,'\\t ',mpjpe_path_3quart_avg,'\\t ',mpjpe_path_avg,'')\n",
    "    print('#'*30)\n",
    "    print('#'*30)\n",
    "\n",
    "    f = open(checkpoint_dir+\"eval.csv\", \"w\")\n",
    "    eval_id = ''\n",
    "    for k,v in vars(args).items():\n",
    "        if k == 'eval_only' or k =='torso_joint' or k =='tag':\n",
    "            continue\n",
    "        eval_id += str(k)+str(v)\n",
    "    eval_result = [eval_id,',',original_tag,',',mpjpe_path_qaurt_avg,',',mpjpe_path_half_avg,',',mpjpe_path_3quart_avg,',',mpjpe_path_avg,',',mpjpe_qaurt_avg,',',mpjpe_half_avg,',',mpjpe_3quart_avg,',',mpjpe_avg,',',FDE,',',ADE,',',STB,'\\n']\n",
    "    eval_result_row = ''\n",
    "    for ss in eval_result:\n",
    "        eval_result_row+= str(ss)\n",
    "    f.write(eval_result_row)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c5c8ad96-8034-4cb2-b94c-ac3bb94839b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load the model weights\n",
      "##############################\n",
      "All results are in mm\n",
      "******************************\n",
      "MPJPE POSE: 0.25\t 0.50\t 0.75\t full\n",
      "MPJPE:  48.2 \t  66.4 \t  73.1 \t  85.3 \n",
      "******************************\n",
      "MPJPE PATH: 0.25\t 0.50\t 0.75\t full\n",
      "PATH:  71.2 \t  106.3 \t  156.1 \t  220.6 \n",
      "##############################\n",
      "##############################\n"
     ]
    }
   ],
   "source": [
    "#Eval at differnet time steps and 3d pose, 3d positions (GTA)\n",
    "\n",
    "print(\"Load the model weights\")\n",
    "model.load_state_dict(torch.load(checkpoint_dir+'val_mpjpe_best.pth'))\n",
    "\n",
    "eval_metrics()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
